{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRS-Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Convert text to lowercase and extract alphanumeric tokens.\n",
    "    This simple tokenizer uses a regular expression to grab word characters.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Use regex to extract tokens containing letters or digits.\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "def build_inverted_index(folder_path):\n",
    "    \"\"\"\n",
    "    Build an inverted index from all .txt files in the given folder.\n",
    "    \n",
    "    The inverted index is a dictionary mapping each term found in the text\n",
    "    to a set of document IDs (assigned sequentially by file processing order).\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(set)\n",
    "    doc_id = 1  # assign document IDs starting at 1\n",
    "\n",
    "    # Process files in a sorted order for reproducibility.\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Tokenize the file content\n",
    "            tokens = tokenize(text)\n",
    "            # Add this doc_id to the set for every token found.\n",
    "            for token in tokens:\n",
    "                inverted_index[token].add(doc_id)\n",
    "\n",
    "            doc_id += 1\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "def print_inverted_index(inverted_index):\n",
    "    \"\"\"\n",
    "    Print the inverted index in the specified format.\n",
    "    \n",
    "    Format:\n",
    "      INVERTED INDEX\n",
    "      ==============\n",
    "      Format: term: doc1, doc2, ...\n",
    "      \n",
    "      <term>: <doc id list>\n",
    "      ...\n",
    "    \"\"\"\n",
    "    print(\"INVERTED INDEX\")\n",
    "    print(\"==============\")\n",
    "    print(\"Format: term: doc1, doc2, ...\\n\")\n",
    "\n",
    "    # Print each term with its document list; sorting tokens for a consistent order.\n",
    "    for term in sorted(inverted_index.keys()):\n",
    "        # Convert the set of doc IDs to a sorted list (ascending numeric order).\n",
    "        doc_ids = sorted(list(inverted_index[term]))\n",
    "        # Join the list of doc IDs into a comma-separated string.\n",
    "        doc_ids_str = \", \".join(str(doc_id) for doc_id in doc_ids)\n",
    "        print(f\"{term}: {doc_ids_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVERTED INDEX\n",
      "==============\n",
      "Format: term: doc1, doc2, ...\n",
      "\n",
      "0: 1, 2, 3\n",
      "00: 2, 3\n",
      "000: 1\n",
      "0002: 1\n",
      "00020: 3\n",
      "001: 2\n",
      "00200: 3\n",
      "00265: 1\n",
      "003: 2\n",
      "01: 2\n",
      "01069: 2\n",
      "02: 2\n",
      "020: 2\n",
      "029: 1\n",
      "03: 1\n",
      "03426: 2\n",
      "03734: 2\n",
      "04: 1\n",
      "05: 2\n",
      "05147: 2\n",
      "05918: 3\n",
      "06: 1, 3\n",
      "07: 2\n",
      "07332: 3\n",
      "07781: 2\n",
      "1: 1, 2, 3\n",
      "10: 1, 2, 3\n",
      "100: 2\n",
      "1000: 1, 3\n",
      "1002: 2\n",
      "1005: 2\n",
      "1007: 2\n",
      "1008: 2\n",
      "1016: 2\n",
      "1024: 1\n",
      "1024_our: 1\n",
      "10k: 3\n",
      "11: 1, 2, 3\n",
      "1106: 2\n",
      "11072: 2\n",
      "1109: 2\n",
      "111: 3\n",
      "11208: 1\n",
      "113: 1, 2\n",
      "1135: 2\n",
      "1137: 1\n",
      "1144: 2\n",
      "1145: 1, 2, 3\n",
      "1149: 1\n",
      "1150: 2\n",
      "1159: 2\n",
      "11929: 3\n",
      "12: 1, 2, 3\n",
      "121: 1\n",
      "123: 1\n",
      "128: 1, 3\n",
      "1281: 2\n",
      "1284: 2\n",
      "13: 1, 2, 3\n",
      "13487: 3\n",
      "135: 2\n",
      "13th: 1\n",
      "14: 1, 2, 3\n",
      "1412: 3\n",
      "1423: 2\n",
      "146: 2\n",
      "1488: 2\n",
      "14899: 3\n",
      "15: 1, 2, 3\n",
      "15th: 1\n",
      "16: 1, 2, 3\n",
      "1602: 3\n",
      "1611: 2\n",
      "1630: 2\n",
      "16th: 1\n",
      "16x16: 3\n",
      "17: 1, 2, 3\n",
      "171111kysb20200002: 1\n",
      "1735: 1\n",
      "1780: 1\n",
      "18: 1, 2, 3\n",
      "1802: 2\n",
      "1804: 2\n",
      "1809: 2\n",
      "18653: 2\n",
      "1893_: 2\n",
      "19: 1, 2, 3\n",
      "1907: 3\n",
      "193: 2\n",
      "1997: 1\n",
      "1http: 2\n",
      "1k: 1\n",
      "1ùëí: 3\n",
      "2: 1, 2, 3\n",
      "20: 1, 2, 3\n",
      "200: 3\n",
      "2005: 3\n",
      "2009: 1\n",
      "200k: 3\n",
      "2010: 2, 3\n",
      "2011: 2\n",
      "2012: 2\n",
      "2014: 1, 3\n",
      "2015: 1\n",
      "2016: 1, 2, 3\n",
      "2017: 1, 2, 3\n",
      "2018: 1, 2, 3\n",
      "2019: 1, 2, 3\n",
      "2020: 1, 2, 3\n",
      "2021: 1, 2, 3\n",
      "2022: 1, 2, 3\n",
      "2048: 1\n",
      "2048_our: 1\n",
      "2048x2048: 1\n",
      "2069: 2\n",
      "2072: 2\n",
      "21: 1, 2, 3\n",
      "2101: 1\n",
      "2102: 3\n",
      "2103: 2, 3\n",
      "2106: 2\n",
      "212: 1\n",
      "21847: 1\n",
      "21856: 1\n",
      "22: 1, 2, 3\n",
      "222: 1\n",
      "2250: 1\n",
      "2261: 1\n",
      "227: 2\n",
      "228: 1\n",
      "22nd: 2\n",
      "23: 1, 2\n",
      "23008: 2\n",
      "24: 1, 2, 3\n",
      "245: 1\n",
      "24963: 2\n",
      "25: 1, 2\n",
      "253: 1\n",
      "256x256: 1\n",
      "25th: 1, 2\n",
      "26: 1, 2, 3\n",
      "263: 2\n",
      "2630: 3\n",
      "2640: 3\n",
      "27: 1, 2, 3\n",
      "272: 2\n",
      "27th: 1\n",
      "28: 1, 2, 3\n",
      "287: 1\n",
      "29: 1, 3\n",
      "2939672: 2\n",
      "2939778: 2\n",
      "2983323: 2\n",
      "2983769: 2\n",
      "299: 2\n",
      "2available: 2\n",
      "2d: 2\n",
      "2ms: 1\n",
      "2x: 1\n",
      "3: 1, 2, 3\n",
      "30: 1, 2, 3\n",
      "300: 1\n",
      "30k: 1\n",
      "31: 1, 3\n",
      "32: 1, 3\n",
      "322: 3\n",
      "3251: 2\n",
      "3252: 2\n",
      "3253: 2\n",
      "3254: 2\n",
      "3255: 2\n",
      "326: 2\n",
      "3289600: 2\n",
      "3290620: 2\n",
      "32g: 3\n",
      "32nd: 1\n",
      "33: 3\n",
      "330: 3\n",
      "332: 2\n",
      "3331184: 2\n",
      "3331312: 2\n",
      "3331377: 2\n",
      "335: 2\n",
      "3397271: 2\n",
      "34: 1\n",
      "3401286: 2\n",
      "345: 1\n",
      "3477495: 2\n",
      "3512527: 3\n",
      "3529466: 1\n",
      "3529486: 1\n",
      "353: 2\n",
      "3531429: 3\n",
      "3531665: 2\n",
      "36: 1, 3\n",
      "365k: 2\n",
      "37: 1\n",
      "374: 3\n",
      "38: 1, 3\n",
      "39: 1\n",
      "390: 3\n",
      "3d: 3\n",
      "3https: 2\n",
      "3rd: 1\n",
      "4: 1, 2, 3\n",
      "40: 1, 2, 3\n",
      "402w: 1\n",
      "408: 2\n",
      "40k: 3\n",
      "40ms: 1\n",
      "41: 3\n",
      "4171: 2\n",
      "4186: 2\n",
      "42: 3\n",
      "4244: 2\n",
      "4250: 2\n",
      "42nd: 2\n",
      "43: 1, 3\n",
      "433: 1\n",
      "43rd: 2\n",
      "44: 1\n",
      "45: 2\n",
      "4503: 1, 2, 3\n",
      "45th: 2\n",
      "46: 3\n",
      "47: 1\n",
      "471: 3\n",
      "4729: 3\n",
      "4765: 2\n",
      "4774: 2\n",
      "48: 1, 3\n",
      "487: 3\n",
      "49: 3\n",
      "4943: 2\n",
      "4951: 2\n",
      "496: 2\n",
      "4https: 2\n",
      "4x: 1\n",
      "5: 1, 2, 3\n",
      "50: 1, 3\n",
      "500: 2\n",
      "5000: 1\n",
      "500w: 1\n",
      "50m: 1\n",
      "51: 1\n",
      "512: 1, 3\n",
      "512_our: 1\n",
      "5139: 2\n",
      "5145: 2\n",
      "52: 1, 3\n",
      "5288: 3\n",
      "5296: 3\n",
      "53: 1, 3\n",
      "54: 1\n",
      "55: 2\n",
      "55th: 2\n",
      "56: 3\n",
      "57: 1, 3\n",
      "574: 2\n",
      "5763: 1\n",
      "5772: 1\n",
      "57th: 2\n",
      "58: 1\n",
      "5803: 3\n",
      "5812: 3\n",
      "59: 1, 3\n",
      "590: 2\n",
      "5998: 3\n",
      "5https: 2\n",
      "6: 1, 2, 3\n",
      "60: 1, 2, 3\n",
      "6008: 3\n",
      "61: 1\n",
      "61732018: 1\n",
      "61802367: 1\n",
      "61872335: 1\n",
      "62: 1, 3\n",
      "63: 1\n",
      "64: 1, 2, 3\n",
      "642: 2\n",
      "6441: 2\n",
      "647: 2\n",
      "65: 1\n",
      "67: 1, 2\n",
      "68: 1\n",
      "69: 3\n",
      "699: 3\n",
      "6ms: 1\n",
      "6th: 1, 2\n",
      "6using: 2\n",
      "6x: 1\n",
      "7: 1, 2, 3\n",
      "70: 1\n",
      "71: 1\n",
      "713: 3\n",
      "714: 2\n",
      "72: 1\n",
      "722: 2\n",
      "727: 2\n",
      "73: 1, 2\n",
      "732k: 2\n",
      "74: 1, 2\n",
      "75: 1\n",
      "76: 1, 2, 3\n",
      "77: 1, 2, 3\n",
      "770: 2\n",
      "7702: 1\n",
      "773: 2\n",
      "78: 1\n",
      "783: 1\n",
      "78ms: 1\n",
      "79: 1\n",
      "8: 1, 2, 3\n",
      "80: 1, 2\n",
      "81: 1\n",
      "82: 1\n",
      "83: 1\n",
      "84: 1\n",
      "85: 1\n",
      "86: 1\n",
      "87: 1\n",
      "8732: 2\n",
      "8746: 3\n",
      "8755: 3\n",
      "88: 1\n",
      "89: 1, 2\n",
      "8a20a8621978632d76c43dfd28b67767: 2\n",
      "8ms: 1\n",
      "9: 1, 2, 3\n",
      "90: 1, 2\n",
      "9016555: 2\n",
      "9073: 2\n",
      "9080: 2\n",
      "91: 1\n",
      "92: 1, 2\n",
      "9238: 3\n",
      "93: 1\n",
      "93918: 3\n",
      "94: 1\n",
      "95: 1\n",
      "9550: 1\n",
      "96: 1\n",
      "97: 1\n",
      "978: 1, 2, 3\n",
      "9781119685265: 2\n",
      "98: 1\n",
      "9826: 3\n",
      "9836: 3\n",
      "999: 2\n",
      "9s: 1\n",
      "a: 1, 2, 3\n",
      "aaai: 2\n",
      "abdulrahman: 2\n",
      "able: 2, 3\n",
      "about: 1, 2, 3\n",
      "above: 1, 2, 3\n",
      "abs: 1, 2, 3\n",
      "absolute: 1\n",
      "abstract: 1, 2, 3\n",
      "abstracting: 2, 3\n",
      "academy: 1\n",
      "acceler: 1\n",
      "accelerate: 1\n",
      "accelerating: 1\n",
      "acceleration: 1\n",
      "accelerator: 1\n",
      "accentuate: 2\n",
      "accept: 1, 2\n",
      "accepting: 2\n",
      "access: 1\n",
      "accessible: 1\n",
      "accompany: 1\n",
      "according: 1, 2\n",
      "account: 2\n",
      "accu: 1\n",
      "accuracy: 1, 3\n",
      "achieve: 1, 3\n",
      "achieves: 1, 3\n",
      "acknowledgments: 1, 2\n",
      "acl: 2\n",
      "acm: 1, 2, 3\n",
      "across: 2\n",
      "actbert: 3\n",
      "action: 3\n",
      "activation: 3\n",
      "activities: 2\n",
      "activity: 2\n",
      "acts: 3\n",
      "actual: 1\n",
      "ad: 2, 3\n",
      "adam: 1\n",
      "adamw: 3\n",
      "adaptive: 1\n",
      "add: 1\n",
      "adding: 2\n",
      "additionally: 2, 3\n",
      "address: 2, 3\n",
      "aditya: 3\n",
      "adjacent: 1\n",
      "adjustment: 1\n",
      "adopt: 2, 3\n",
      "adopted: 1, 2\n",
      "advances: 1, 2, 3\n",
      "advantage: 1, 2, 3\n",
      "advantages: 1\n",
      "affect: 2\n",
      "affecting: 2\n",
      "affinity: 1\n",
      "after: 1, 3\n",
      "ag: 2\n",
      "agarwal: 3\n",
      "age: 1\n",
      "agent: 3\n",
      "aglar: 1\n",
      "agnostic: 2, 3\n",
      "agree: 2\n",
      "agreement: 2\n",
      "ah: 1\n",
      "ahead: 1\n",
      "ahmed: 3\n",
      "ai: 1, 2\n",
      "aidan: 1, 3\n",
      "aided: 2\n",
      "aids: 2\n",
      "aim: 1\n",
      "aimed: 2\n",
      "aims: 1\n",
      "al: 1, 2, 3\n",
      "alan: 1\n",
      "alayrac: 3\n",
      "albanie: 3\n",
      "alec: 3\n",
      "alexander: 3\n",
      "alexey: 3\n",
      "alexis: 2\n",
      "algo: 1\n",
      "algorithm: 1, 3\n",
      "algorithmic: 1\n",
      "algorithms: 3\n",
      "alice: 1\n",
      "align: 1, 3\n",
      "aligned: 1, 2, 3\n",
      "alignment: 1\n",
      "alignments: 3\n",
      "all: 1, 2, 3\n",
      "allocate: 1\n",
      "allocating: 1\n",
      "allowed: 1\n",
      "allows: 3\n",
      "almost: 1, 2\n",
      "alona: 2\n",
      "along: 2, 3\n",
      "also: 1, 2, 3\n",
      "alternative: 1\n",
      "although: 2\n",
      "alto: 3\n",
      "alvin: 2\n",
      "amanda: 3\n",
      "amato: 1\n",
      "amd: 1\n",
      "american: 1, 2\n",
      "among: 1, 3\n",
      "amount: 1\n",
      "an: 1, 2, 3\n",
      "analogies: 2\n",
      "analyses: 1\n",
      "analysis: 1, 2\n",
      "analytics: 2\n",
      "analyze: 1\n",
      "anand: 2\n",
      "and: 1, 2, 3\n",
      "anderson: 1\n",
      "andrea: 1\n",
      "andreas: 2\n",
      "andrew: 1, 3\n",
      "anna: 2\n",
      "anne: 3\n",
      "anno: 2\n",
      "annotate: 2\n",
      "annotated: 1, 2\n",
      "annotation: 2\n",
      "annotations: 2, 3\n",
      "annual: 1, 2\n",
      "anonymous: 2\n",
      "answer: 2\n",
      "answering: 1, 2, 3\n",
      "antoine: 3\n",
      "any: 2, 3\n",
      "aochun: 1\n",
      "aohua: 3\n",
      "applica: 3\n",
      "application: 1\n",
      "applications: 1, 2, 3\n",
      "applies: 1\n",
      "apply: 1, 3\n",
      "applying: 3\n",
      "approach: 1, 2\n",
      "approaches: 1, 3\n",
      "appropriate: 1, 3\n",
      "approxi: 2\n",
      "approximated: 2\n",
      "approximately: 1\n",
      "approximating: 2\n",
      "approximation: 2\n",
      "april: 1, 2\n",
      "ar: 1\n",
      "archi: 3\n",
      "architecture: 1, 3\n",
      "are: 1, 2, 3\n",
      "area: 1\n",
      "areas: 1\n",
      "argued: 2\n",
      "arithmetic: 1\n",
      "armand: 2\n",
      "arrow: 3\n",
      "arsha: 3\n",
      "art: 1, 3\n",
      "article: 2\n",
      "artificial: 1, 2\n",
      "arxiv: 2, 3\n",
      "as: 1, 2, 3\n",
      "ashish: 1, 3\n",
      "ask: 2\n",
      "asked: 2\n",
      "askell: 3\n",
      "aspect: 2\n",
      "aspects: 1, 2\n",
      "asserts: 2\n",
      "assessment: 2\n",
      "assign: 1\n",
      "asso: 1\n",
      "assoc: 1, 2\n",
      "associ: 2\n",
      "associates: 2\n",
      "association: 1, 2, 3\n",
      "associations: 2\n",
      "at: 1, 2, 3\n",
      "ate: 1\n",
      "ation: 1\n",
      "ations: 2\n",
      "ative: 1\n",
      "atlas: 1\n",
      "atlas300: 1\n",
      "atom: 1\n",
      "attempted: 1\n",
      "attention: 1, 2, 3\n",
      "attentive: 2\n",
      "attn: 3\n",
      "attribution: 1\n",
      "audi: 2\n",
      "august: 1, 2\n",
      "aurelio: 2\n",
      "australia: 2\n",
      "austrian: 1\n",
      "author: 1\n",
      "automate: 1\n",
      "automotive: 2\n",
      "available: 1\n",
      "ave: 1\n",
      "average: 1, 2, 3\n",
      "averaging: 1\n",
      "avishek: 2\n",
      "aware: 1\n",
      "axis: 3\n",
      "az: 2\n",
      "b: 1, 2\n",
      "b60: 2\n",
      "back: 1, 3\n",
      "background: 2\n",
      "backpropagation: 1\n",
      "bah: 1\n",
      "bailey: 3\n",
      "balance: 1\n",
      "bansal: 3\n",
      "baptiste: 3\n",
      "barbara: 2\n",
      "barcelona: 2\n",
      "bart: 1\n",
      "basaj: 2\n",
      "based: 1, 2, 3\n",
      "baseline: 3\n",
      "bases: 2\n",
      "basic: 1\n",
      "batch: 1, 3\n",
      "bc: 2\n",
      "be: 1, 2, 3\n",
      "beach: 1\n",
      "bear: 2, 3\n",
      "because: 3\n",
      "becomes: 2\n",
      "been: 2, 3\n",
      "before: 1, 3\n",
      "behaviour: 2\n",
      "being: 2\n",
      "belongie: 1\n",
      "ben: 1\n",
      "benchmark: 3\n",
      "benefits: 2, 3\n",
      "bengio: 1, 2\n",
      "benjamin: 2\n",
      "benoit: 1\n",
      "berg: 3\n",
      "bernstein: 3\n",
      "bert: 1, 2\n",
      "besides: 3\n",
      "best: 1, 2, 3\n",
      "better: 1, 2, 3\n",
      "between: 1, 2, 3\n",
      "beyer: 3\n",
      "beyond: 2\n",
      "biased: 2\n",
      "bidirectional: 1, 2\n",
      "biecek: 2\n",
      "big: 2\n",
      "bilingual: 2\n",
      "billions: 2\n",
      "binary: 3\n",
      "bite: 2\n",
      "black: 2\n",
      "blindly: 2\n",
      "blue: 1\n",
      "bmvc: 1\n",
      "bo: 1\n",
      "bojanowski: 2\n",
      "both: 1, 2, 3\n",
      "bottom: 1\n",
      "bougares: 1\n",
      "boundaries: 2\n",
      "bowen: 3\n",
      "box: 1, 2\n",
      "boxes: 1\n",
      "boyd: 2\n",
      "boyle: 1\n",
      "branch: 3\n",
      "branches: 3\n",
      "braschler: 2\n",
      "bridging: 3\n",
      "bring: 2\n",
      "british: 1\n",
      "broader: 2\n",
      "bruce: 2\n",
      "brute: 3\n",
      "bryan: 3\n",
      "buehler: 1\n",
      "build: 2, 3\n",
      "building: 1\n",
      "builds: 3\n",
      "bulk: 2\n",
      "burstein: 2\n",
      "business: 2\n",
      "but: 1, 2, 3\n",
      "by: 1, 2, 3\n",
      "c: 1\n",
      "ca: 1, 2, 3\n",
      "calculate: 1\n",
      "calculation: 1\n",
      "calculations: 1\n",
      "call: 1\n",
      "calls: 2\n",
      "cam: 3\n",
      "cambrian: 1\n",
      "camp: 1\n",
      "can: 1, 2, 3\n",
      "canada: 2\n",
      "canlong: 1\n",
      "canonical: 1\n",
      "caption: 3\n",
      "captioning: 1\n",
      "captions: 1, 3\n",
      "capture: 2\n",
      "captures: 2\n",
      "card: 1\n",
      "cards: 1\n",
      "caren: 2\n",
      "carlos: 2\n",
      "carol: 2\n",
      "carried: 2\n",
      "cas: 1\n",
      "case: 2\n",
      "category: 3\n",
      "catenating: 3\n",
      "cause: 1\n",
      "causes: 1\n",
      "cc: 2\n",
      "ccs: 1, 2, 3\n",
      "ce: 3\n",
      "center: 3\n",
      "cess: 1\n",
      "cessing: 2\n",
      "ch17: 2\n",
      "challenge: 3\n",
      "challenges: 1, 3\n",
      "challenging: 3\n",
      "chang: 1, 2\n",
      "changing: 3\n",
      "channels: 3\n",
      "chao: 3\n",
      "chapter: 1, 2\n",
      "characteristic: 1\n",
      "characteristics: 1\n",
      "che: 2\n",
      "chen: 1, 3\n",
      "cheng: 1, 2, 3\n",
      "chenhao: 1\n",
      "chetana: 2\n",
      "chevalier: 2\n",
      "china: 1, 2\n",
      "chinese: 1\n",
      "chios: 2\n",
      "chiuman: 3\n",
      "cho: 1\n",
      "choi: 1, 2\n",
      "choice: 1\n",
      "choose: 1\n",
      "chris: 1, 3\n",
      "christy: 2\n",
      "chun: 1, 3\n",
      "chung: 1\n",
      "chunyuan: 1\n",
      "ciation: 1\n",
      "ciency: 3\n",
      "cikm: 2\n",
      "cimiano: 2\n",
      "circuits: 1\n",
      "citation: 2, 3\n",
      "cited: 3\n",
      "city: 1\n",
      "claim: 2\n",
      "claimed: 3\n",
      "claims: 2\n",
      "clark: 3\n",
      "class: 3\n",
      "classical: 1\n",
      "classification: 1, 2, 3\n",
      "classified: 3\n",
      "classifier: 2\n",
      "classroom: 2, 3\n",
      "clip: 3\n",
      "clipbert: 3\n",
      "clips: 3\n",
      "closer: 1\n",
      "closest: 1, 3\n",
      "cloud: 1\n",
      "clough: 2\n",
      "cls: 3\n",
      "clude: 1\n",
      "cnn: 1\n",
      "co: 1, 2\n",
      "coco: 1\n",
      "coded: 3\n",
      "collaborative: 1, 3\n",
      "collected: 1\n",
      "collection: 2\n",
      "colors: 3\n",
      "com: 1, 2, 3\n",
      "combines: 1\n",
      "combining: 1, 3\n",
      "comes: 1\n",
      "commercial: 1, 2, 3\n",
      "commission: 2\n",
      "commit: 1\n",
      "common: 1, 2\n",
      "commons: 1\n",
      "compare: 1, 2\n",
      "compared: 1, 3\n",
      "comparison: 1, 2, 3\n",
      "comparisons: 1\n",
      "competing: 2\n",
      "competitor: 2\n",
      "completeness: 2\n",
      "completenessùëû: 2\n",
      "complex: 1, 2, 3\n",
      "complexity: 1\n",
      "components: 2, 3\n",
      "composed: 1\n",
      "comprehensible: 2\n",
      "comprehension: 2\n",
      "comprehensive: 2\n",
      "comprehensively: 2\n",
      "compu: 1\n",
      "comput: 1, 2\n",
      "computa: 1\n",
      "computation: 1, 3\n",
      "computational: 1, 2\n",
      "computations: 3\n",
      "compute: 2\n",
      "computed: 2, 3\n",
      "computer: 1, 3\n",
      "computes: 2\n",
      "computing: 1, 2, 3\n",
      "compùë§: 2\n",
      "con: 1, 2, 3\n",
      "concat: 3\n",
      "concatenate: 3\n",
      "concatenated: 2, 3\n",
      "concatenating: 3\n",
      "concatenation: 3\n",
      "concepts: 1, 2, 3\n",
      "concise: 1\n",
      "conclude: 2\n",
      "conclusion: 1, 2, 3\n",
      "concurrently: 1\n",
      "conduct: 3\n",
      "conducted: 2\n",
      "conducting: 2\n",
      "conference: 1, 2, 3\n",
      "configuration: 1\n",
      "configurations: 1\n",
      "conjunc: 2\n",
      "conneau: 2\n",
      "connected: 1\n",
      "connecting: 3\n",
      "connection: 1\n",
      "consensus: 2\n",
      "consequently: 3\n",
      "consider: 1, 2, 3\n",
      "considerable: 1\n",
      "considerably: 1, 2\n",
      "considered: 2\n",
      "considering: 1\n",
      "consists: 1, 2\n",
      "conspicuously: 1\n",
      "constrained: 1\n",
      "construct: 1\n",
      "constructed: 1, 3\n",
      "consumed: 3\n",
      "consuming: 1\n",
      "consumption: 1\n",
      "cont: 2\n",
      "contain: 1, 3\n",
      "contained: 1\n",
      "containing: 1\n",
      "contains: 1, 3\n",
      "content: 1, 2\n",
      "context: 1\n",
      "contextual: 2\n",
      "contrastive: 3\n",
      "contribute: 2\n",
      "contributed: 2\n",
      "contribution: 2\n",
      "contributions: 1, 3\n",
      "conv: 1\n",
      "conventional: 3\n",
      "convert: 2, 3\n",
      "converted: 3\n",
      "convolution: 1\n",
      "convolutional: 1\n",
      "cooper: 1\n",
      "coopera: 1\n",
      "copies: 2, 3\n",
      "copy: 2, 3\n",
      "copyright: 1\n",
      "copyrights: 2, 3\n",
      "cor: 2\n",
      "corpora: 2\n",
      "corpus: 2, 3\n",
      "corr: 1, 2, 3\n",
      "correct: 1\n",
      "correlation: 1\n",
      "correspond: 1\n",
      "correspondences: 1\n",
      "corresponding: 1, 3\n",
      "corresponds: 1\n",
      "cosine: 2\n",
      "cosine_similarity: 2\n",
      "cost: 1, 2\n",
      "costs: 1\n",
      "could: 2\n",
      "cpu: 1\n",
      "creative: 1\n",
      "credit: 2, 3\n",
      "critical: 1\n",
      "croft: 2\n",
      "cross: 1, 2, 3\n",
      "crossvit: 3\n",
      "crowdsourced: 3\n",
      "curious: 2\n",
      "current: 1, 3\n",
      "currently: 1\n",
      "custom: 1\n",
      "cution: 1\n",
      "cvf: 1, 3\n",
      "cvpr: 1, 3\n",
      "cybern: 2\n",
      "d: 1, 2\n",
      "damien: 1\n",
      "danau: 1\n",
      "darrell: 3\n",
      "dashed: 3\n",
      "data: 1, 2, 3\n",
      "datak: 2\n",
      "dataset: 1, 3\n",
      "datasets: 1, 3\n",
      "david: 1, 3\n",
      "day: 3\n",
      "de: 1, 2\n",
      "dealing: 1\n",
      "debasis: 2\n",
      "december: 1\n",
      "decision: 2\n",
      "decoder: 1\n",
      "decrease: 1\n",
      "dedicated: 1\n",
      "dedicatedly: 1\n",
      "deep: 1, 2, 3\n",
      "deepshap: 2\n",
      "default: 3\n",
      "defined: 1, 3\n",
      "dehghani: 3\n",
      "delay: 1\n",
      "deluca: 2\n",
      "demo: 2\n",
      "demonstrate: 1, 2, 3\n",
      "demonstrated: 3\n",
      "demonstrates: 3\n",
      "denotations: 1\n",
      "denote: 1\n",
      "denoted: 2\n",
      "denoyer: 2\n",
      "dense: 2, 3\n",
      "dependent: 3\n",
      "deploying: 1\n",
      "deployment: 1\n",
      "depth: 3\n",
      "desai: 3\n",
      "describe: 1\n",
      "described: 3\n",
      "description: 1, 3\n",
      "descriptions: 1, 2, 3\n",
      "descriptor: 3\n",
      "descriptors: 1\n",
      "design: 1, 2\n",
      "designed: 1, 3\n",
      "designs: 1\n",
      "desired: 1\n",
      "detailed: 1, 2\n",
      "details: 3\n",
      "detec: 1\n",
      "detected: 1\n",
      "detection: 1\n",
      "detectors: 1\n",
      "determine: 2\n",
      "determines: 1\n",
      "deva: 1\n",
      "develop: 1\n",
      "development: 1, 2\n",
      "devices: 1\n",
      "devlin: 1, 2\n",
      "didemo: 3\n",
      "diego: 1\n",
      "dif: 2\n",
      "differences: 2\n",
      "different: 1, 2, 3\n",
      "differently: 1, 2\n",
      "difficult: 2\n",
      "difficulty: 1\n",
      "digital: 2, 3\n",
      "dimen: 1\n",
      "dimension: 1, 2, 3\n",
      "dimensional: 1, 2\n",
      "dimensions: 1\n",
      "dimitri: 3\n",
      "ding: 2\n",
      "dings: 3\n",
      "direction: 2\n",
      "directly: 1, 3\n",
      "dirk: 3\n",
      "disagree: 2\n",
      "discovery: 2\n",
      "discriminate: 2\n",
      "discussed: 2\n",
      "distance: 1\n",
      "distribute: 1\n",
      "distributed: 2, 3\n",
      "distributional: 2\n",
      "divide: 2\n",
      "divide1: 2\n",
      "dmitry: 1\n",
      "dnn: 1\n",
      "dnns: 1\n",
      "do: 1, 2, 3\n",
      "doc: 2\n",
      "docu: 2\n",
      "document: 2\n",
      "documents: 2\n",
      "does: 1, 3\n",
      "doha: 1\n",
      "doi: 1, 2, 3\n",
      "doll: 1\n",
      "domain: 1, 2, 3\n",
      "dominika: 2\n",
      "domly: 3\n",
      "donahue: 3\n",
      "done: 3\n",
      "dong: 1\n",
      "dongrui: 1\n",
      "doran: 2\n",
      "dosovitskiy: 3\n",
      "dot: 1\n",
      "down: 1\n",
      "downstream: 2\n",
      "dozens: 1\n",
      "draw: 2\n",
      "driven: 2\n",
      "dsran: 1\n",
      "dual: 1, 3\n",
      "ducted: 3\n",
      "due: 1, 3\n",
      "duerig: 3\n",
      "during: 1, 3\n",
      "dynamic: 2\n",
      "dynamically: 1\n",
      "dzmitry: 1\n",
      "e: 3\n",
      "eaai: 2\n",
      "each: 1, 2, 3\n",
      "earlier: 2\n",
      "easily: 1, 2\n",
      "eccv: 1, 3\n",
      "eckert: 2\n",
      "ed: 2\n",
      "edge: 1\n",
      "edouard: 2\n",
      "eds: 2\n",
      "educational: 2\n",
      "ef: 1\n",
      "effect: 2\n",
      "effective: 2\n",
      "effi: 3\n",
      "efficiency: 1\n",
      "efficient: 1, 3\n",
      "effort: 2\n",
      "ehre: 1\n",
      "eighth: 2\n",
      "either: 2\n",
      "el: 3\n",
      "elements: 1, 2\n",
      "eli: 3\n",
      "eliminate: 1\n",
      "elkhatib: 1\n",
      "emb: 1\n",
      "embed: 3\n",
      "embedded: 1\n",
      "embedder: 2\n",
      "embedding: 1, 3\n",
      "embeddings: 1, 2\n",
      "emnlp: 1\n",
      "emphasize: 3\n",
      "empirical: 1\n",
      "employ: 2\n",
      "employed: 1\n",
      "employs: 1\n",
      "empowering: 2\n",
      "en: 2, 3\n",
      "enable: 1\n",
      "enables: 2\n",
      "ence: 1\n",
      "encode: 1, 3\n",
      "encoded: 1, 3\n",
      "encoder: 1, 3\n",
      "encoding: 1, 2, 3\n",
      "end: 1, 2\n",
      "energy: 1\n",
      "eng: 2\n",
      "engine: 2\n",
      "engineers: 2\n",
      "engines: 2\n",
      "english: 2\n",
      "enhance: 1\n",
      "enhanced: 1\n",
      "enriching: 2\n",
      "ensured: 2\n",
      "entire: 3\n",
      "entropy: 3\n",
      "epo: 2\n",
      "epochs: 1, 2, 3\n",
      "epyc: 1\n",
      "eq: 3\n",
      "eqn: 2\n",
      "equal: 1\n",
      "equals: 3\n",
      "equation: 1\n",
      "equations: 1\n",
      "equivalent: 3\n",
      "eric: 2\n",
      "ernesto: 2\n",
      "ernst: 2\n",
      "erogeneous: 1\n",
      "especially: 1\n",
      "essential: 1, 3\n",
      "establish: 1\n",
      "established: 1\n",
      "establishes: 1\n",
      "esuli: 1\n",
      "et: 1, 2, 3\n",
      "etc: 1\n",
      "ethics: 2\n",
      "eu: 2\n",
      "europa: 2\n",
      "european: 1, 2, 3\n",
      "evalu: 1\n",
      "evaluate: 1, 2, 3\n",
      "evaluated: 3\n",
      "evaluating: 1, 2\n",
      "evaluation: 1, 2, 3\n",
      "evaluations: 2\n",
      "even: 2, 3\n",
      "event: 1\n",
      "ever: 2\n",
      "every: 1, 3\n",
      "evident: 2\n",
      "evidently: 1\n",
      "ex: 1, 2\n",
      "exact: 1\n",
      "example: 1\n",
      "excellent: 1\n",
      "except: 1, 3\n",
      "exchange: 3\n",
      "exchanged: 3\n",
      "exe: 1\n",
      "executed: 1\n",
      "executes: 1\n",
      "execution: 1\n",
      "existing: 1, 3\n",
      "exp01: 2\n",
      "exp02: 2\n",
      "exp03: 2\n",
      "exp04: 2\n",
      "expect: 2\n",
      "expectations: 2\n",
      "experimental: 1\n",
      "experiments: 1, 3\n",
      "experts: 3\n",
      "explain: 2\n",
      "explainability: 2\n",
      "explainable: 2\n",
      "explainer: 2\n",
      "explaining: 2\n",
      "explains: 2\n",
      "explanation: 2\n",
      "explanations: 2, 3\n",
      "explicit: 3\n",
      "exploited: 2\n",
      "exploiting: 2\n",
      "explore: 1\n",
      "explored: 2\n",
      "explores: 2\n",
      "expose: 2\n",
      "exposing: 2\n",
      "expres: 1\n",
      "expressed: 2\n",
      "expression: 1\n",
      "exs: 2\n",
      "extend: 1, 3\n",
      "extended: 2\n",
      "extending: 3\n",
      "extrac: 1\n",
      "extract: 1, 3\n",
      "extracted: 1, 3\n",
      "extracting: 1, 3\n",
      "extraction: 1, 3\n",
      "extractor: 1, 3\n",
      "extremely: 3\n",
      "extrinsic: 2\n",
      "f: 1\n",
      "f1: 2\n",
      "fabrizio: 1\n",
      "face: 1\n",
      "facebookresearch: 2\n",
      "faces: 1, 2\n",
      "faceted: 1\n",
      "facets: 2\n",
      "facilitate: 3\n",
      "factor: 2\n",
      "factors: 2\n",
      "factorùë§as: 2\n",
      "faghri: 1\n",
      "fail: 2\n",
      "fair: 1, 2\n",
      "faisal: 3\n",
      "falchi: 1\n",
      "falk: 2\n",
      "familiar: 2\n",
      "fan: 1, 2, 3\n",
      "fang: 1\n",
      "far: 1, 2\n",
      "fartash: 1\n",
      "fast: 1, 3\n",
      "faster: 1, 3\n",
      "fasttext: 2\n",
      "fasttext4: 2\n",
      "favorably: 1\n",
      "fccm: 1\n",
      "fea: 3\n",
      "feature: 1, 2, 3\n",
      "features: 1, 3\n",
      "february: 2\n",
      "fed: 3\n",
      "fee: 2, 3\n",
      "feed: 2, 3\n",
      "fei: 3\n",
      "feng: 1, 2\n",
      "ferent: 2\n",
      "fergus: 2\n",
      "fernando: 2\n",
      "fethi: 1\n",
      "few: 1\n",
      "fewer: 1\n",
      "ffg: 1\n",
      "ffn: 3\n",
      "ficiency: 1\n",
      "fidler: 1\n",
      "field: 1, 2\n",
      "fig: 2, 3\n",
      "figure: 1, 2, 3\n",
      "filled: 1\n",
      "filter: 1\n",
      "final: 1, 2, 3\n",
      "finally: 1, 2\n",
      "financial: 2\n",
      "find: 1\n",
      "finer: 3\n",
      "first: 1, 2, 3\n",
      "firstname: 2\n",
      "five: 1\n",
      "flatten: 3\n",
      "fleet: 1\n",
      "flickr: 1, 3\n",
      "flickr30k: 1\n",
      "fliker30k: 1\n",
      "florence: 2\n",
      "flow: 1\n",
      "flowchart: 3\n",
      "focus: 1\n",
      "focusing: 2\n",
      "folds: 1\n",
      "follow: 3\n",
      "followed: 3\n",
      "following: 1, 3\n",
      "follows: 1\n",
      "for: 1, 2, 3\n",
      "force: 3\n",
      "foreign: 2\n",
      "form: 1, 2, 3\n",
      "format: 1, 2, 3\n",
      "formed: 3\n",
      "former: 1\n",
      "formula: 1\n",
      "forum: 2\n",
      "forward: 1, 2, 3\n",
      "found: 2, 3\n",
      "foundation: 1\n",
      "four: 1\n",
      "fourth: 2\n",
      "fp: 1\n",
      "fpgas: 1\n",
      "fraction: 1\n",
      "fragment: 1\n",
      "frame: 1, 3\n",
      "frames: 3\n",
      "framework: 1, 3\n",
      "france: 1, 2\n",
      "francisco: 2\n",
      "franco: 1\n",
      "freezing: 3\n",
      "frequency: 2\n",
      "from: 1, 2, 3\n",
      "frozen: 3\n",
      "fse: 3\n",
      "fu: 1, 3\n",
      "full: 1, 2, 3\n",
      "fully: 1, 3\n",
      "function: 3\n",
      "functions: 3\n",
      "fundamental: 1\n",
      "further: 1, 2\n",
      "furu: 1\n",
      "fuse: 3\n",
      "fuses: 1\n",
      "fusing: 3\n",
      "fusion: 1, 3\n",
      "future: 1, 2\n",
      "g: 1, 3\n",
      "gabriel: 3\n",
      "gabriele: 1\n",
      "gan: 3\n",
      "gang: 1\n",
      "ganguly: 2\n",
      "gao: 1\n",
      "gap: 2\n",
      "gareth: 2\n",
      "garnett: 2\n",
      "gat: 1\n",
      "gated: 1, 3\n",
      "gather: 2\n",
      "gating: 2\n",
      "gaussier: 2\n",
      "gcf: 3\n",
      "gcn: 1\n",
      "gelly: 3\n",
      "general: 1, 2\n",
      "generally: 1\n",
      "generate: 1, 3\n",
      "generated: 2\n",
      "generates: 1\n",
      "generation: 3\n",
      "genome: 3\n",
      "gensim: 2\n",
      "geometric: 1\n",
      "georg: 2, 3\n",
      "ger: 2\n",
      "german: 2\n",
      "germany: 1, 2\n",
      "girish: 3\n",
      "girshick: 1\n",
      "github: 2\n",
      "giuseppe: 1\n",
      "give: 1\n",
      "given: 1, 2, 3\n",
      "gives: 2\n",
      "glasgow: 1\n",
      "global: 1, 2, 3\n",
      "gnn: 1\n",
      "goh: 3\n",
      "gomez: 1, 3\n",
      "gonter: 2\n",
      "good: 1\n",
      "gori: 1\n",
      "gould: 1\n",
      "gpu: 1, 3\n",
      "gpus: 1, 3\n",
      "graber: 2\n",
      "gram: 1\n",
      "grant: 1\n",
      "granted: 2, 3\n",
      "graph: 1\n",
      "grave: 2\n",
      "greatly: 1\n",
      "green: 1\n",
      "gregg: 1\n",
      "gretchen: 3\n",
      "grissom: 2\n",
      "groth: 3\n",
      "ground: 1\n",
      "gru: 1\n",
      "gsls: 1\n",
      "gtx2080ti: 1\n",
      "gu: 1\n",
      "guage: 2\n",
      "gual: 2\n",
      "guangzhou: 1\n",
      "guericke: 2\n",
      "guestrin: 2\n",
      "guide: 3\n",
      "guided: 3\n",
      "guidelines: 2\n",
      "guiding: 3\n",
      "guillaume: 2\n",
      "guistics: 2\n",
      "gunhee: 3\n",
      "guo: 1, 2\n",
      "guyon: 2\n",
      "had: 2\n",
      "hagenbuchner: 1\n",
      "hallacy: 3\n",
      "halved: 1\n",
      "hamish: 2\n",
      "han: 2\n",
      "hand: 1\n",
      "handling: 2\n",
      "hanjalic: 1\n",
      "hanna: 2\n",
      "hao: 2\n",
      "hard: 1, 2, 3\n",
      "harder: 2\n",
      "hardware: 1\n",
      "hartwig: 1\n",
      "has: 1, 2, 3\n",
      "hash: 2\n",
      "hata: 3\n",
      "have: 1, 2, 3\n",
      "hays: 1\n",
      "he: 1, 2\n",
      "head: 1, 3\n",
      "healy: 2\n",
      "heavy: 1\n",
      "heigold: 3\n",
      "held: 1\n",
      "help: 2\n",
      "helped: 2\n",
      "helpful: 2\n",
      "helpfulness: 2\n",
      "helping: 2\n",
      "helps: 1, 3\n",
      "hence: 3\n",
      "hendricks: 3\n",
      "heng: 1\n",
      "hereby: 2\n",
      "hero: 3\n",
      "het: 1\n",
      "heterogeneous: 1\n",
      "hexiang: 3\n",
      "hierarchical: 2, 3\n",
      "hieu: 3\n",
      "high: 1, 2\n",
      "higher: 2\n",
      "highest: 1, 3\n",
      "highly: 3\n",
      "himself: 2\n",
      "hlt: 1\n",
      "ho: 3\n",
      "hoc: 2\n",
      "hochreiter: 1\n",
      "hockenmaier: 1\n",
      "hodosh: 1\n",
      "holger: 1\n",
      "homner: 2\n",
      "hongsheng: 1\n",
      "honored: 2, 3\n",
      "hop: 2\n",
      "hopefully: 1\n",
      "hou: 2\n",
      "houdong: 1\n",
      "hover: 2\n",
      "how: 2\n",
      "howard: 1\n",
      "however: 1, 2\n",
      "howto100m: 3\n",
      "hsiao: 3\n",
      "ht: 3\n",
      "htm: 2\n",
      "html: 2\n",
      "http: 2\n",
      "https: 1, 2, 3\n",
      "hu: 1, 3\n",
      "hua: 1\n",
      "huanbo: 2\n",
      "huang: 2\n",
      "huawei: 1\n",
      "huei: 1\n",
      "huge: 2\n",
      "huifang: 1\n",
      "huijuan: 3\n",
      "human: 1\n",
      "hundred: 3\n",
      "hyper: 3\n",
      "hypothesis: 2\n",
      "i: 1, 2, 3\n",
      "iaai: 2\n",
      "iccv: 1, 3\n",
      "iciai: 1\n",
      "iclr: 1, 2\n",
      "icmr: 3\n",
      "icoin: 2\n",
      "icoin48656: 2\n",
      "icpr: 1\n",
      "icrtit: 2\n",
      "id: 2\n",
      "idea: 2\n",
      "ideas: 2\n",
      "identified: 2\n",
      "identifying: 2\n",
      "ieee: 1, 2, 3\n",
      "if: 2, 3\n",
      "ii: 2\n",
      "iii: 2\n",
      "ijcai: 2\n",
      "illia: 1, 3\n",
      "illustrated: 2\n",
      "ilya: 3\n",
      "im: 1\n",
      "image: 1, 3\n",
      "images: 1, 3\n",
      "impacts: 1\n",
      "imperative: 1\n",
      "imple: 1\n",
      "implement: 1\n",
      "implementation: 1\n",
      "implements: 1\n",
      "importance: 2\n",
      "important: 3\n",
      "impractical: 3\n",
      "improve: 1, 2\n",
      "improvement: 3\n",
      "improves: 1, 2\n",
      "improving: 1\n",
      "in: 1, 2, 3\n",
      "inal: 3\n",
      "includ: 2\n",
      "include: 1, 3\n",
      "includes: 1\n",
      "including: 1\n",
      "increase: 1\n",
      "increased: 1\n",
      "increases: 1\n",
      "independent: 3\n",
      "independently: 3\n",
      "index: 2, 3\n",
      "indexes: 1\n",
      "indexing: 1\n",
      "indianapolis: 2\n",
      "individual: 2\n",
      "individually: 2\n",
      "industrial: 1\n",
      "inevitable: 2\n",
      "infer: 1\n",
      "inference: 1, 3\n",
      "inferred: 1\n",
      "infinite: 2\n",
      "influence: 1\n",
      "information: 1, 2, 3\n",
      "informed: 2\n",
      "ing: 1, 2\n",
      "ingolstadt: 2\n",
      "inherently: 2\n",
      "initial: 1, 3\n",
      "initially: 2\n",
      "inner: 3\n",
      "innovation: 1\n",
      "innovative: 2\n",
      "input: 1, 2, 3\n",
      "inputs: 3\n",
      "inputting: 1\n",
      "inspired: 1, 3\n",
      "instances: 1, 2\n",
      "instead: 1, 3\n",
      "institute: 1, 2\n",
      "int: 2\n",
      "integer: 1\n",
      "intelligence: 1, 2\n",
      "inter: 2, 3\n",
      "interaction: 3\n",
      "interchange: 3\n",
      "interchanged: 3\n",
      "interface: 2\n",
      "interna: 1\n",
      "international: 1, 2, 3\n",
      "internet: 2\n",
      "interpretability: 2\n",
      "interpretable: 2\n",
      "interpretations: 2\n",
      "interpreted: 3\n",
      "interpreting: 2\n",
      "into: 2, 3\n",
      "intriguing: 2\n",
      "intrinsic: 2\n",
      "introduction: 1, 2, 3\n",
      "intuitive: 3\n",
      "investigate: 2\n",
      "involv: 2\n",
      "involve: 1\n",
      "involved: 2\n",
      "involving: 1\n",
      "ioannis: 2\n",
      "ir: 2\n",
      "irrelevant: 2\n",
      "is: 1, 2, 3\n",
      "isabelle: 2\n",
      "isbn: 1, 2, 3\n",
      "isnext: 2\n",
      "isolated: 3\n",
      "issue: 2\n",
      "issues: 3\n",
      "it: 1, 2, 3\n",
      "italy: 1, 2\n",
      "item: 1\n",
      "itr: 1\n",
      "its: 2\n",
      "iv: 1\n",
      "ivan: 3\n",
      "ivison: 2\n",
      "iyyer: 2\n",
      "ization: 1\n",
      "j: 1, 2\n",
      "jaap: 2\n",
      "jack: 3\n",
      "jacob: 1, 2\n",
      "jaeho: 2\n",
      "jakob: 1, 3\n",
      "james: 1, 3\n",
      "jamie: 1\n",
      "january: 1, 2\n",
      "jaspreet: 2\n",
      "jean: 3\n",
      "jecting: 2\n",
      "jeff: 3\n",
      "jenhao: 3\n",
      "jf: 2\n",
      "ji: 2\n",
      "jia: 3\n",
      "jiafeng: 2\n",
      "jian: 1, 2\n",
      "jianfeng: 1\n",
      "jiang: 1\n",
      "jianxin: 2\n",
      "jiawei: 3\n",
      "jie: 1, 3\n",
      "jill: 2\n",
      "jimmy: 2\n",
      "jinfeng: 3\n",
      "jing: 1\n",
      "jingjing: 1, 3\n",
      "jingkuan: 1\n",
      "john: 2\n",
      "johnson: 1, 3\n",
      "joint: 1, 2, 3\n",
      "jointly: 1\n",
      "jones: 1, 2, 3\n",
      "jong: 3\n",
      "jongseok: 3\n",
      "joongheon: 2\n",
      "jordan: 2\n",
      "josef: 3\n",
      "joshua: 3\n",
      "josiah: 2\n",
      "joulin: 2\n",
      "jsfusion: 3\n",
      "judge: 2\n",
      "judgement: 2\n",
      "judges: 2\n",
      "judging: 2\n",
      "julia: 1\n",
      "july: 2\n",
      "jun: 3\n",
      "june: 1, 3\n",
      "junjie: 1\n",
      "just: 2\n",
      "justin: 3\n",
      "j√©r√¥me: 2\n",
      "k: 1, 3\n",
      "kai: 1\n",
      "kaiming: 1\n",
      "kaiser: 1, 3\n",
      "kalantidis: 3\n",
      "kalenichenko: 1\n",
      "kamps: 2\n",
      "karan: 3\n",
      "karen: 1\n",
      "kate: 3\n",
      "keeping: 3\n",
      "kenji: 3\n",
      "kenton: 1, 2\n",
      "key: 2\n",
      "keyframe: 1\n",
      "keyu: 1\n",
      "keywords: 1, 2, 3\n",
      "kholy: 3\n",
      "kim: 2, 3\n",
      "kind: 2\n",
      "kinds: 1, 3\n",
      "kiros: 1\n",
      "kligys: 1\n",
      "knowl: 2\n",
      "knowledge: 1, 2\n",
      "kolesnikov: 3\n",
      "korea: 1\n",
      "korra: 2\n",
      "kraus: 2\n",
      "kravitz: 3\n",
      "krishna: 3\n",
      "kristina: 1, 2\n",
      "krueger: 3\n",
      "krug: 2\n",
      "kuang: 1\n",
      "kumar: 2\n",
      "kunpeng: 1\n",
      "kyunghyun: 1\n",
      "kyusong: 1\n",
      "l: 3\n",
      "l2: 1\n",
      "label: 2, 3\n",
      "labelling: 2\n",
      "labels: 3\n",
      "labs: 2\n",
      "lai: 1\n",
      "lake: 1\n",
      "lample: 2\n",
      "lan: 2\n",
      "lang: 2\n",
      "language: 1, 2, 3\n",
      "languages: 2, 3\n",
      "laptev: 3\n",
      "large: 1, 2, 3\n",
      "largely: 2\n",
      "larger: 1\n",
      "largest: 1\n",
      "las: 1\n",
      "last: 3\n",
      "lastname: 2\n",
      "latency: 1\n",
      "later: 2\n",
      "lawrence: 1\n",
      "layer: 1, 3\n",
      "layers: 1, 3\n",
      "le: 1, 3\n",
      "lead: 3\n",
      "learn: 1, 2\n",
      "learned: 1, 2, 3\n",
      "learning: 1, 2, 3\n",
      "learning5: 2\n",
      "learningvia: 3\n",
      "learns: 1\n",
      "lecture: 1\n",
      "lee: 1, 2\n",
      "lei: 1, 3\n",
      "leibniz: 2\n",
      "leland: 2\n",
      "length: 2, 3\n",
      "less: 1, 2, 3\n",
      "level: 1\n",
      "leverage: 3\n",
      "leveraging: 3\n",
      "li: 1, 2, 3\n",
      "li1: 3\n",
      "liang: 1\n",
      "license: 1\n",
      "licensed: 1\n",
      "licheng: 3\n",
      "life: 1\n",
      "light: 1, 2\n",
      "lightningdot: 1\n",
      "lightweight: 1\n",
      "lihong: 2\n",
      "lijuan: 1\n",
      "like: 3\n",
      "lime: 2\n",
      "limitations: 1\n",
      "limited: 1\n",
      "lin: 1, 2\n",
      "linchao: 3\n",
      "line: 1, 3\n",
      "linear: 1, 2, 3\n",
      "ling: 1, 2\n",
      "lingual: 2\n",
      "linguistics: 1, 2\n",
      "linjie: 1, 3\n",
      "liqiang: 1\n",
      "lirme: 2\n",
      "lisa: 3\n",
      "lish: 2\n",
      "list: 3\n",
      "listed: 3\n",
      "lists: 2, 3\n",
      "liu: 1, 2, 3\n",
      "li–¥ht: 1\n",
      "llion: 1, 3\n",
      "ln: 3\n",
      "load: 1\n",
      "local: 1, 2, 3\n",
      "localizing: 3\n",
      "locally: 2\n",
      "long: 1, 2, 3\n",
      "longer: 3\n",
      "loss: 3\n",
      "losses: 3\n",
      "lot: 1, 3\n",
      "low: 1\n",
      "lower: 1\n",
      "lstm: 1\n",
      "lu: 1\n",
      "luan: 2\n",
      "luca: 2\n",
      "lucas: 3\n",
      "ludovic: 2\n",
      "lukasz: 1\n",
      "lundberg: 2\n",
      "luo: 2, 3\n",
      "luowei: 3\n",
      "luxburg: 2\n",
      "lùê∂: 3\n",
      "lùëÜ: 3\n",
      "m: 2\n",
      "ma: 1, 3\n",
      "maarek: 2\n",
      "macao: 2\n",
      "mach: 2\n",
      "machine: 1, 2\n",
      "machinery: 2, 3\n",
      "machines: 1, 2\n",
      "macro: 2\n",
      "madarapu: 2\n",
      "made: 2, 3\n",
      "madrid: 2\n",
      "magdeburg: 2\n",
      "main: 1, 3\n",
      "mainly: 1, 3\n",
      "mains: 2\n",
      "maintaining: 3\n",
      "maire: 1\n",
      "makarand: 3\n",
      "make: 1, 2, 3\n",
      "makes: 1, 2, 3\n",
      "making: 1\n",
      "man: 2\n",
      "management: 2\n",
      "manifold: 2\n",
      "manisha: 2\n",
      "manner: 3\n",
      "manual: 1\n",
      "mao: 2\n",
      "maosong: 2\n",
      "map: 1\n",
      "maps: 3\n",
      "marc: 2\n",
      "march: 1\n",
      "marco: 1, 2\n",
      "marcus: 3\n",
      "mark: 1, 2, 3\n",
      "marked: 2\n",
      "market: 1\n",
      "markus: 1\n",
      "martin: 2\n",
      "masked: 2\n",
      "match: 1\n",
      "matching: 1, 2\n",
      "mation: 2\n",
      "matrix: 1, 3\n",
      "matthew: 1\n",
      "matthias: 3\n",
      "max: 2\n",
      "may: 1, 2, 3\n",
      "mb: 1\n",
      "mcinnes: 2\n",
      "mdm: 1\n",
      "mdr: 3\n",
      "mea: 1\n",
      "mean: 1, 2, 3\n",
      "means: 3\n",
      "meanwhile: 1\n",
      "measure: 1\n",
      "mecha: 1, 3\n",
      "mechanism: 1, 3\n",
      "mechanisms: 1\n",
      "media: 1, 2\n",
      "median: 1, 3\n",
      "meet: 1\n",
      "meeting: 2\n",
      "meets: 1\n",
      "mei: 3\n",
      "melbourne: 2\n",
      "memory: 1, 3\n",
      "menglong: 1\n",
      "ment: 1, 2\n",
      "mented: 1\n",
      "mentioned: 2, 3\n",
      "merrienboer: 1\n",
      "message: 1\n",
      "messina: 1\n",
      "method: 1, 3\n",
      "methodologies: 1, 2\n",
      "methodology: 3\n",
      "methods: 1, 2, 3\n",
      "metric: 2\n",
      "metrics: 1, 2, 3\n",
      "mfm: 1\n",
      "mhca: 3\n",
      "mhsa: 3\n",
      "micah: 1\n",
      "michael: 1, 2, 3\n",
      "microsoft: 1\n",
      "miech: 3\n",
      "mikolov: 2\n",
      "milan: 1\n",
      "million: 3\n",
      "milliseconds: 1\n",
      "mimic: 1\n",
      "min: 1, 2\n",
      "minderer: 3\n",
      "ming: 1, 2\n",
      "mingxing: 1\n",
      "mini: 1\n",
      "mining: 2\n",
      "minneapolis: 1\n",
      "mishkin: 3\n",
      "mishra: 2\n",
      "mlir: 2\n",
      "mm: 1\n",
      "mn: 1\n",
      "mnasnet: 1\n",
      "mnr: 3\n",
      "mobile: 1\n",
      "mobility: 2\n",
      "mod: 3\n",
      "modal: 1, 3\n",
      "modalities: 3\n",
      "modality: 1\n",
      "model: 1, 2, 3\n",
      "modeling: 3\n",
      "modelling: 2\n",
      "models: 1, 2, 3\n",
      "modify: 3\n",
      "module: 1, 2, 3\n",
      "modules: 1, 2\n",
      "mohit: 2, 3\n",
      "moments: 3\n",
      "monfardini: 1\n",
      "monolingual: 2\n",
      "mooney: 3\n",
      "more: 1, 2, 3\n",
      "moreover: 3\n",
      "most: 1, 2\n",
      "mostafa: 3\n",
      "mother: 2\n",
      "mounted: 1\n",
      "ms: 1\n",
      "mscoco: 1\n",
      "msr: 3\n",
      "msrvtt: 3\n",
      "mtfn: 1\n",
      "much: 1, 2, 3\n",
      "mul: 3\n",
      "multi: 1, 2, 3\n",
      "multilin: 2\n",
      "multilingual: 2\n",
      "multilingualism: 2\n",
      "multimedia: 1, 3\n",
      "multimodal: 3\n",
      "multiple: 1, 2, 3\n",
      "multiplication: 1\n",
      "munich: 1\n",
      "murdock: 2\n",
      "muse: 2\n",
      "must: 2, 3\n",
      "mutual: 3\n",
      "n: 1, 2, 3\n",
      "n19: 2\n",
      "naacl: 1\n",
      "nagrani: 3\n",
      "nan: 1\n",
      "napa: 1\n",
      "naresh: 2\n",
      "narrated: 3\n",
      "national: 1\n",
      "natural: 1, 2, 3\n",
      "naturally: 3\n",
      "nature: 2\n",
      "necessary: 1\n",
      "nected: 1\n",
      "need: 1, 2, 3\n",
      "needs: 1, 3\n",
      "negatively: 2\n",
      "negatives: 1\n",
      "negligible: 1\n",
      "net: 1, 2, 3\n",
      "network: 1, 2, 3\n",
      "networking: 2\n",
      "networks: 1, 3\n",
      "neural: 1, 2, 3\n",
      "neurips: 2\n",
      "neurocomputing: 1\n",
      "neutral: 2\n",
      "new: 1, 2, 3\n",
      "newark: 3\n",
      "newcastle: 1\n",
      "newly: 1, 3\n",
      "next: 1, 2\n",
      "nice: 1\n",
      "niche: 2\n",
      "nicola: 1\n",
      "nie: 2\n",
      "niki: 1, 3\n",
      "nina: 2\n",
      "nism: 1, 3\n",
      "nj: 3\n",
      "nlp: 2\n",
      "no: 1, 2, 3\n",
      "noam: 1, 3\n",
      "node: 1\n",
      "nodes: 1\n",
      "noisy: 3\n",
      "non: 1, 3\n",
      "none: 2\n",
      "norbert: 2\n",
      "normalization: 3\n",
      "north: 1, 2\n",
      "not: 1, 2, 3\n",
      "note: 2\n",
      "noted: 2\n",
      "notes: 1\n",
      "notice: 2, 3\n",
      "noticeable: 1\n",
      "noting: 2\n",
      "notion: 2\n",
      "novel: 3\n",
      "november: 1\n",
      "now: 2\n",
      "nowadays: 2\n",
      "nsp: 2\n",
      "number: 1, 2, 3\n",
      "nv: 1\n",
      "ny: 1, 2, 3\n",
      "o: 1\n",
      "object: 1\n",
      "objects: 1\n",
      "observe: 3\n",
      "observed: 2\n",
      "obtain: 3\n",
      "obtained: 1, 2\n",
      "obtains: 1\n",
      "occupied: 1\n",
      "occupies: 1\n",
      "occupy: 1\n",
      "occurrence: 2\n",
      "occurrences: 2\n",
      "october: 1, 2\n",
      "of: 1, 2, 3\n",
      "off: 1\n",
      "offers: 1\n",
      "office3: 2\n",
      "offline: 1\n",
      "often: 1, 2, 3\n",
      "oh: 2\n",
      "ojs: 2\n",
      "oliver: 3\n",
      "omitted: 2\n",
      "omni: 3\n",
      "on: 1, 2, 3\n",
      "once: 2\n",
      "one: 1, 2, 3\n",
      "ongoing: 2\n",
      "online: 1\n",
      "only: 1, 2, 3\n",
      "openreview: 2\n",
      "operation: 1, 3\n",
      "operations: 1\n",
      "operators: 1\n",
      "oppo: 3\n",
      "optimal: 1, 3\n",
      "optimiza: 1\n",
      "optimization: 1\n",
      "optimizations: 1\n",
      "optimize: 1\n",
      "optimized: 1\n",
      "optimizer: 1, 3\n",
      "optimizing: 1\n",
      "option: 2\n",
      "or: 1, 2, 3\n",
      "order: 2\n",
      "org: 1, 2, 3\n",
      "organizations: 2\n",
      "orig: 3\n",
      "original: 1, 3\n",
      "oscar: 1\n",
      "other: 1, 2, 3\n",
      "others: 2, 3\n",
      "otherwise: 2, 3\n",
      "otto: 2\n",
      "our: 1, 2, 3\n",
      "ours: 3\n",
      "out: 2, 3\n",
      "outline: 1\n",
      "outperform: 3\n",
      "outperforms: 2\n",
      "output: 1, 3\n",
      "outputs: 3\n",
      "over: 1, 2\n",
      "overall: 1, 2\n",
      "overview: 1, 2\n",
      "ovgu: 2\n",
      "owned: 2, 3\n",
      "owner: 1\n",
      "p: 1\n",
      "p17: 2\n",
      "p19: 2\n",
      "pad: 1\n",
      "page: 2, 3\n",
      "pages: 1, 2, 3\n",
      "pair: 1, 3\n",
      "paired: 1, 3\n",
      "pairs: 1, 2, 3\n",
      "pairwise: 2\n",
      "palo: 3\n",
      "pamela: 3\n",
      "panda: 3\n",
      "pang: 1\n",
      "paper: 1, 2, 3\n",
      "papers: 2, 3\n",
      "parallel: 1, 2\n",
      "parallelism: 1\n",
      "parameter: 1, 3\n",
      "parameters: 1, 2, 3\n",
      "parekh: 3\n",
      "paris: 2\n",
      "parmar: 1, 3\n",
      "part: 1, 2, 3\n",
      "participants: 2\n",
      "particularly: 2\n",
      "parts: 1\n",
      "passed: 2\n",
      "passing: 1, 3\n",
      "patch: 3\n",
      "patent: 2\n",
      "patents: 2\n",
      "pathologies: 2\n",
      "paths: 1\n",
      "pattern: 1, 3\n",
      "paul: 2\n",
      "pedro: 2\n",
      "peng: 2\n",
      "pengchuan: 1\n",
      "per: 1, 2\n",
      "perceive: 2\n",
      "perform: 1, 2\n",
      "performance: 1, 2, 3\n",
      "performances: 2\n",
      "performed: 1\n",
      "performing: 2\n",
      "performs: 1, 3\n",
      "periments: 1\n",
      "permission: 2, 3\n",
      "permissions: 2, 3\n",
      "permitted: 2, 3\n",
      "perona: 1\n",
      "personal: 2, 3\n",
      "perturbed: 2\n",
      "peter: 1\n",
      "peters: 2\n",
      "petitor: 2\n",
      "pham: 3\n",
      "phased: 2\n",
      "phases: 2\n",
      "philipp: 2\n",
      "philosophy: 1\n",
      "php: 2\n",
      "phrase: 1\n",
      "picture: 2\n",
      "piece: 1\n",
      "pietro: 1\n",
      "ping: 1\n",
      "piotr: 1, 2\n",
      "pipeline: 3\n",
      "pipline: 3\n",
      "piwowarski: 2\n",
      "placing: 1\n",
      "planations: 2\n",
      "platform: 1\n",
      "platforms: 1\n",
      "plays: 1, 3\n",
      "plus: 3\n",
      "points: 1, 2\n",
      "polarity: 2\n",
      "polosukhin: 1, 3\n",
      "polysemous: 2\n",
      "pool: 3\n",
      "pooling: 3\n",
      "poon: 2\n",
      "populate: 1\n",
      "pora: 2\n",
      "porto: 1\n",
      "portugal: 1\n",
      "pose: 2\n",
      "positively: 2\n",
      "possible: 3\n",
      "post: 2, 3\n",
      "pothula: 2\n",
      "power: 1\n",
      "practical: 1, 2, 3\n",
      "practice: 2\n",
      "pradeepta: 2\n",
      "pre: 1, 2, 3\n",
      "precision: 1, 2\n",
      "predic: 2\n",
      "predict: 2\n",
      "prediction: 2\n",
      "predictions: 2\n",
      "preferences: 2\n",
      "preferred: 2\n",
      "preform: 2\n",
      "premise: 2\n",
      "preprint: 2, 3\n",
      "present: 1, 2\n",
      "presented: 2, 3\n",
      "presents: 1, 2\n",
      "press: 2\n",
      "presumably: 2\n",
      "previous: 1, 3\n",
      "primary: 1\n",
      "primitive: 1\n",
      "prior: 2, 3\n",
      "priority: 1\n",
      "pro: 1, 2\n",
      "probabilities: 2\n",
      "probability: 3\n",
      "problem: 1, 3\n",
      "procedure: 1, 3\n",
      "proceedings: 1, 2, 3\n",
      "process: 1, 2, 3\n",
      "processed: 1\n",
      "processes: 1\n",
      "processing: 1, 2, 3\n",
      "procheta: 2\n",
      "producing: 2\n",
      "product: 1, 3\n",
      "professional: 2\n",
      "proficient: 2\n",
      "profit: 2, 3\n",
      "programmable: 1\n",
      "programming: 1\n",
      "prohibi: 3\n",
      "project: 1, 2, 3\n",
      "projected: 2, 3\n",
      "projecting: 3\n",
      "projection: 1, 2, 3\n",
      "promote: 1\n",
      "propagation: 1\n",
      "proportion: 1\n",
      "proposal: 1\n",
      "propose: 1, 2, 3\n",
      "proposed: 1, 2, 3\n",
      "proposes: 1\n",
      "provide: 2\n",
      "provided: 2, 3\n",
      "proving: 1\n",
      "pruning: 1\n",
      "przemyslaw: 2\n",
      "pseudo: 3\n",
      "pt: 3\n",
      "purple: 1\n",
      "purpose: 1\n",
      "pursuit: 1\n",
      "put: 2, 3\n",
      "pute: 1\n",
      "puts: 1\n",
      "python: 2\n",
      "pytorch: 3\n",
      "p√∂rner: 2\n",
      "qa: 2\n",
      "qatar: 1\n",
      "qianren: 2\n",
      "qingcheng: 1\n",
      "qingrong: 1\n",
      "qingyao: 2\n",
      "qualitative: 1, 2\n",
      "quality: 2\n",
      "quanfu: 3\n",
      "quantitative: 1\n",
      "quantities: 1\n",
      "quantity: 1\n",
      "quantization: 1\n",
      "quantum: 2\n",
      "queries: 1, 2, 3\n",
      "query: 1, 2, 3\n",
      "question: 1, 2, 3\n",
      "quite: 1\n",
      "quoc: 1, 3\n",
      "r: 1, 3\n",
      "racy: 1\n",
      "radford: 3\n",
      "radimrehurek: 2\n",
      "radu: 1\n",
      "raju: 2\n",
      "ramanan: 1\n",
      "ramesh: 3\n",
      "rameswar: 3\n",
      "ran: 3\n",
      "ranjay: 3\n",
      "rank: 2, 3\n",
      "ranked: 2\n",
      "ranking: 1, 2\n",
      "ranzato: 2\n",
      "rapid: 2\n",
      "rate: 1, 2, 3\n",
      "rated: 2\n",
      "ratings: 2\n",
      "raymond: 3\n",
      "rcnn: 1\n",
      "re: 1, 2, 3\n",
      "rea: 1\n",
      "reach: 1\n",
      "reaches: 1\n",
      "read: 3\n",
      "reading: 2, 3\n",
      "real: 1, 3\n",
      "realize: 1\n",
      "realizing: 1\n",
      "reason: 1, 2\n",
      "reasoning: 1\n",
      "recall: 1, 2, 3\n",
      "recalls: 1\n",
      "recent: 2\n",
      "recently: 1, 2, 3\n",
      "recognition: 1, 3\n",
      "recompute: 3\n",
      "recurrent: 1, 3\n",
      "recursive: 1\n",
      "red: 1\n",
      "redistribute: 2, 3\n",
      "reduce: 1, 3\n",
      "reduced: 1\n",
      "reducing: 1\n",
      "reduction: 1, 2\n",
      "redundancy: 1\n",
      "reference: 1, 2, 3\n",
      "references: 1, 2, 3\n",
      "refers: 1\n",
      "refine: 1\n",
      "refining: 1\n",
      "reflection: 2\n",
      "regard: 2\n",
      "regarded: 3\n",
      "regardless: 2\n",
      "region: 1\n",
      "regional: 1\n",
      "regions: 1\n",
      "regular: 2\n",
      "rela: 1\n",
      "related: 1, 2, 3\n",
      "relation: 1\n",
      "relational: 1\n",
      "relations: 1\n",
      "relationship: 1\n",
      "relative: 2\n",
      "relatively: 1, 2\n",
      "release_ip: 2\n",
      "relevance: 1, 2\n",
      "relevant: 2, 3\n",
      "reliable: 1\n",
      "ren: 1\n",
      "rep: 1, 3\n",
      "repeat: 3\n",
      "repre: 3\n",
      "represent: 3\n",
      "representation: 1, 2, 3\n",
      "representations: 1, 2, 3\n",
      "represented: 3\n",
      "represents: 1, 3\n",
      "republish: 2, 3\n",
      "request: 2, 3\n",
      "require: 1, 3\n",
      "required: 1, 3\n",
      "requirement: 1\n",
      "requirements: 1\n",
      "requires: 1, 2, 3\n",
      "research: 1, 2, 3\n",
      "researchers: 3\n",
      "researches: 1, 3\n",
      "resentations: 1, 3\n",
      "residual: 1, 3\n",
      "resnet: 1\n",
      "resource: 1\n",
      "resources: 1, 2\n",
      "respectively: 1, 3\n",
      "responded: 2\n",
      "responses: 2\n",
      "rest: 1\n",
      "result: 1, 2, 3\n",
      "results: 1, 2, 3\n",
      "retrieval: 1, 2, 3\n",
      "retrievals: 1\n",
      "retrieve: 1, 2\n",
      "retrieved: 1, 2\n",
      "retriever: 2\n",
      "retrieves: 3\n",
      "retrieving: 1, 2, 3\n",
      "returned: 2\n",
      "returning: 2\n",
      "returns: 2\n",
      "rex: 2\n",
      "rex2: 2\n",
      "ribeiro: 2\n",
      "rightly: 2\n",
      "rising: 2\n",
      "rithms: 1\n",
      "rkyttf: 2\n",
      "rnn: 1\n",
      "rob: 2\n",
      "robust: 2, 3\n",
      "rodriguez: 2\n",
      "rohrbach: 3\n",
      "role: 1, 3\n",
      "roman: 2\n",
      "rong: 2\n",
      "ross: 1\n",
      "roth: 2\n",
      "roughly: 1\n",
      "rows: 1\n",
      "rui: 3\n",
      "ruiping: 1\n",
      "run: 1, 3\n",
      "running: 1\n",
      "runtime: 1\n",
      "ruoming: 1\n",
      "russell: 3\n",
      "rvw–¥: 1\n",
      "ryan: 1\n",
      "rychalska: 2\n",
      "s: 1, 2\n",
      "s13042: 2\n",
      "s2vt: 3\n",
      "sabine: 2\n",
      "sacrificing: 1\n",
      "saenko: 3\n",
      "salt: 1\n",
      "same: 1, 2, 3\n",
      "sameer: 2\n",
      "sample: 2\n",
      "sampled: 2\n",
      "samples: 1, 2\n",
      "sampling: 2, 3\n",
      "samuel: 3\n",
      "samy: 2\n",
      "san: 1, 2\n",
      "sandhini: 3\n",
      "sandler: 1\n",
      "sanja: 1\n",
      "sanz: 1\n",
      "sarit: 2\n",
      "sastry: 3\n",
      "satiate: 2\n",
      "satisfactory: 1\n",
      "satisfying: 1\n",
      "save: 3\n",
      "sbac: 1\n",
      "scale: 1, 3\n",
      "scaling: 3\n",
      "scan: 1\n",
      "scarselli: 1\n",
      "scenario: 1\n",
      "scenarios: 1\n",
      "scene: 1\n",
      "scenes: 1\n",
      "scheme: 1, 3\n",
      "schmidhuber: 1\n",
      "scholer: 2\n",
      "schwenk: 1\n",
      "science: 1\n",
      "sciences: 1\n",
      "scientists: 1\n",
      "score: 2\n",
      "scores: 2\n",
      "scott: 2\n",
      "scraped: 2\n",
      "search: 1, 2, 3\n",
      "searchers: 1\n",
      "searches: 2\n",
      "searching: 1, 2\n",
      "sebastian: 2\n",
      "second: 1, 2, 3\n",
      "section: 2, 3\n",
      "security: 1\n",
      "see: 2, 3\n",
      "seen: 2\n",
      "sees: 2\n",
      "select: 2\n",
      "selected: 2, 3\n",
      "selecting: 1\n",
      "selection: 1\n",
      "selective: 1\n",
      "selectively: 1\n",
      "self: 1, 3\n",
      "semantic: 1, 3\n",
      "semantically: 2\n",
      "semantics: 1, 2\n",
      "sen: 2\n",
      "sent: 3\n",
      "sentence: 1, 2, 3\n",
      "sentences: 1, 2\n",
      "sentiment: 2\n",
      "senzhang: 2\n",
      "seoul: 1\n",
      "separate: 1\n",
      "separately: 1, 3\n",
      "sepp: 1\n",
      "september: 1\n",
      "sequen: 1\n",
      "sequence: 3\n",
      "sequences: 3\n",
      "sequentially: 2\n",
      "serge: 1\n",
      "series: 1\n",
      "serve: 1, 2\n",
      "server: 1\n",
      "servers: 1, 2, 3\n",
      "serves: 1\n",
      "session: 3\n",
      "set: 1, 2, 3\n",
      "sets: 1, 2\n",
      "setting: 1, 3\n",
      "settings: 3\n",
      "seunghyeok: 2\n",
      "seventh: 2\n",
      "several: 1, 3\n",
      "sgm: 1\n",
      "sha: 3\n",
      "shamma: 3\n",
      "shan: 1\n",
      "shao: 1\n",
      "shaoqing: 1\n",
      "shapley: 2\n",
      "sharing: 3\n",
      "shazeer: 1, 3\n",
      "shechtman: 3\n",
      "shen: 1\n",
      "sheng: 1\n",
      "shengen: 1\n",
      "shi: 2\n",
      "shiguang: 1\n",
      "short: 1, 3\n",
      "shortcut: 3\n",
      "should: 2, 3\n",
      "show: 1, 3\n",
      "showed: 2\n",
      "shown: 1, 2, 3\n",
      "shows: 1, 2, 3\n",
      "shuohang: 1\n",
      "sidered: 2\n",
      "sidige: 2\n",
      "sigir: 2\n",
      "sigkdd: 2\n",
      "signed: 2\n",
      "significance: 2\n",
      "significant: 1, 2\n",
      "significantly: 1\n",
      "sijin: 1\n",
      "similar: 1, 2\n",
      "similarities: 3\n",
      "similarity: 1, 2, 3\n",
      "similarity6: 2\n",
      "simonyan: 1\n",
      "simple: 1\n",
      "simply: 3\n",
      "simultaneously: 3\n",
      "since: 1, 2, 3\n",
      "singh: 2\n",
      "single: 1, 3\n",
      "sion: 1, 3\n",
      "siqi: 1\n",
      "site: 1\n",
      "sivic: 3\n",
      "siwen: 2\n",
      "size: 1, 3\n",
      "sizes: 1\n",
      "skirmantas: 1\n",
      "sklearn: 2\n",
      "slightly: 1\n",
      "slow: 3\n",
      "small: 1\n",
      "smaller: 1\n",
      "snowmass: 1\n",
      "so: 1, 3\n",
      "sociations: 2\n",
      "softmax: 3\n",
      "software: 1\n",
      "solid: 3\n",
      "solorio: 2\n",
      "solve: 1\n",
      "some: 3\n",
      "song: 1\n",
      "soning: 1\n",
      "sorg: 2\n",
      "sorting: 1\n",
      "sota: 3\n",
      "south: 1\n",
      "soyeon: 2\n",
      "space: 1, 2, 3\n",
      "spaces: 2\n",
      "spain: 2\n",
      "sparse: 1, 3\n",
      "spatial: 3\n",
      "speak: 2\n",
      "special: 1, 3\n",
      "specialized: 1\n",
      "specific: 1, 2, 3\n",
      "specifically: 1\n",
      "specified: 1\n",
      "spectively: 1\n",
      "spectrum: 2\n",
      "sped: 1\n",
      "speed: 1, 3\n",
      "speedup: 1\n",
      "speedups: 3\n",
      "split: 1, 3\n",
      "splits: 1\n",
      "springer: 1, 2\n",
      "st: 2\n",
      "stack: 3\n",
      "stacked: 1\n",
      "stage: 1, 3\n",
      "standability: 2\n",
      "standard: 1, 2\n",
      "stat: 2\n",
      "state: 1, 3\n",
      "static: 2\n",
      "statistical: 1\n",
      "statistically: 2\n",
      "statistics: 2\n",
      "stephanie: 3\n",
      "stephen: 1\n",
      "steps: 3\n",
      "still: 1\n",
      "stockholm: 2\n",
      "strategic: 1\n",
      "strategies: 2\n",
      "strategy: 3\n",
      "strengthen: 1\n",
      "strictly: 1\n",
      "strongly: 2\n",
      "struc: 1\n",
      "structure: 1, 2\n",
      "structures: 1\n",
      "studied: 1\n",
      "study: 1, 2\n",
      "studying: 1\n",
      "su: 2\n",
      "sub: 1, 3\n",
      "subhashini: 3\n",
      "subsequent: 1\n",
      "subword: 2\n",
      "success: 3\n",
      "such: 1, 2, 3\n",
      "sudhi: 2\n",
      "suffers: 1, 3\n",
      "suggest: 3\n",
      "suggested: 2\n",
      "suitable: 1\n",
      "sujatha: 2\n",
      "summary: 3\n",
      "sun: 1, 2\n",
      "sung: 3\n",
      "superior: 1\n",
      "supervised: 3\n",
      "supervision: 3\n",
      "support: 1\n",
      "supported: 1, 2\n",
      "supporting: 1\n",
      "supports: 2\n",
      "sure: 1\n",
      "surpassing: 2\n",
      "surprisingly: 3\n",
      "survey: 2\n",
      "sutskever: 3\n",
      "suzan: 2\n",
      "sweden: 2\n",
      "switzerland: 1\n",
      "sydorova: 2\n",
      "sylvain: 3\n",
      "sym: 3\n",
      "symmetric: 3\n",
      "symposium: 1, 2\n",
      "sys: 1, 2\n",
      "syst: 1\n",
      "system: 1, 2, 3\n",
      "systems: 1, 2, 3\n",
      "table: 1, 2, 3\n",
      "tacl: 2\n",
      "tains: 3\n",
      "take: 3\n",
      "takes: 1, 2, 3\n",
      "taking: 1, 2\n",
      "tamara: 3\n",
      "tan: 1\n",
      "tang: 1\n",
      "tao: 1, 3\n",
      "tapaswi: 3\n",
      "targeted: 1\n",
      "task: 1, 2, 3\n",
      "tasks: 1, 2, 3\n",
      "taso: 1\n",
      "tated: 2\n",
      "tation: 1\n",
      "taylor: 1\n",
      "techniques: 2\n",
      "technol: 1\n",
      "technologies: 1\n",
      "technology: 1, 2\n",
      "tected: 1\n",
      "tection: 1\n",
      "tecture: 3\n",
      "tem: 1, 2\n",
      "temperature: 3\n",
      "temporal: 3\n",
      "ten: 1\n",
      "tences: 2\n",
      "tend: 2\n",
      "teney: 1\n",
      "tensor: 1\n",
      "tenth: 2\n",
      "tention: 1\n",
      "term: 1, 2\n",
      "terminal: 1\n",
      "terms: 1, 2\n",
      "ternational: 1\n",
      "test: 1, 2\n",
      "tested: 1, 3\n",
      "testing: 1, 3\n",
      "tex: 1\n",
      "text: 1, 2, 3\n",
      "texts: 1\n",
      "textual: 3\n",
      "thamar: 2\n",
      "than: 1, 2, 3\n",
      "thank: 2\n",
      "that: 1, 2, 3\n",
      "the: 1, 2, 3\n",
      "theguardian: 2\n",
      "their: 1, 2\n",
      "them: 2\n",
      "themes: 1\n",
      "then: 1, 2, 3\n",
      "there: 1, 2, 3\n",
      "thereby: 2\n",
      "therefore: 1, 2, 3\n",
      "these: 1, 2\n",
      "they: 1, 2\n",
      "things: 1\n",
      "think: 2\n",
      "thinking: 3\n",
      "third: 3\n",
      "thirty: 2\n",
      "this: 1, 2, 3\n",
      "thomas: 3\n",
      "thoroughly: 1\n",
      "those: 2, 3\n",
      "though: 2, 3\n",
      "three: 1, 2\n",
      "through: 1\n",
      "throughput: 1\n",
      "thus: 1, 3\n",
      "tially: 1\n",
      "tiancheng: 1\n",
      "time: 1, 3\n",
      "times: 1, 3\n",
      "ting: 2, 3\n",
      "tion: 1, 2\n",
      "tional: 1\n",
      "tions: 1, 2, 3\n",
      "tionship: 1\n",
      "tiple: 3\n",
      "tive: 1\n",
      "tively: 3\n",
      "to: 1, 2, 3\n",
      "token: 3\n",
      "tokens: 3\n",
      "tom: 3\n",
      "tom√°s: 2\n",
      "tongue: 2\n",
      "too: 1\n",
      "tools: 3\n",
      "top: 1\n",
      "total: 1, 2, 3\n",
      "toutanova: 1, 2\n",
      "towards: 1, 2\n",
      "tpu: 1\n",
      "track: 2\n",
      "traction: 1\n",
      "trades: 1\n",
      "traditional: 3\n",
      "train: 2, 3\n",
      "trained: 1, 2, 3\n",
      "training: 1, 2, 3\n",
      "trans: 1, 2, 3\n",
      "transacl: 2\n",
      "transactions: 1\n",
      "transferable: 3\n",
      "transferred: 1\n",
      "transferring: 1\n",
      "transfers: 1\n",
      "transformation: 1\n",
      "transformer: 1, 3\n",
      "transformers: 1, 2, 3\n",
      "transforms: 2\n",
      "translating: 3\n",
      "translation: 1, 2\n",
      "transmission: 1\n",
      "transparency: 2\n",
      "traversing: 3\n",
      "treat: 2\n",
      "trends: 2\n",
      "trevor: 2, 3\n",
      "tried: 2\n",
      "trieval: 1, 2, 3\n",
      "trivial: 3\n",
      "trust: 2\n",
      "trustworthy: 2\n",
      "truth: 1\n",
      "tsoi: 1\n",
      "tsung: 1\n",
      "tu: 2\n",
      "tuitive: 2\n",
      "ture: 1\n",
      "tures: 3\n",
      "turing: 2\n",
      "twelfth: 2\n",
      "twenty: 2\n",
      "two: 1, 2, 3\n",
      "t√∫lio: 2\n",
      "tŒ∏: 1\n",
      "uk: 1\n",
      "ulc: 1\n",
      "ule: 3\n",
      "ulrike: 2\n",
      "umap: 2\n",
      "ument: 2\n",
      "un: 2\n",
      "unaffected: 1\n",
      "unanimously: 2\n",
      "unbiased: 2\n",
      "under: 1, 2\n",
      "underlying: 2\n",
      "understand: 2\n",
      "understandability: 2\n",
      "understandable: 2\n",
      "understanding: 1, 2\n",
      "uni: 2\n",
      "unified: 2\n",
      "uniform: 2\n",
      "unique: 1\n",
      "unit: 1\n",
      "uniter: 3\n",
      "units: 1\n",
      "universal: 3\n",
      "university: 2\n",
      "unlabelled: 3\n",
      "unmatched: 1\n",
      "unpublished: 1\n",
      "unsatisfactory: 3\n",
      "unsupervised: 2\n",
      "unterthiner: 3\n",
      "up: 1, 3\n",
      "update: 1, 3\n",
      "upon: 3\n",
      "urgen: 1\n",
      "us: 2, 3\n",
      "usa: 1, 2, 3\n",
      "usage: 1\n",
      "use: 1, 2, 3\n",
      "used: 2, 3\n",
      "useful: 2\n",
      "usefulness: 2\n",
      "user: 2\n",
      "users: 1, 2\n",
      "uses: 1, 3\n",
      "using: 1, 2, 3\n",
      "usually: 1, 2\n",
      "uszkoreit: 1, 3\n",
      "ut: 1\n",
      "utilize: 3\n",
      "utilized: 1, 3\n",
      "utilizes: 3\n",
      "v: 1, 2, 3\n",
      "v1: 1, 2\n",
      "v100: 1, 3\n",
      "valentin: 1\n",
      "validation: 1\n",
      "values: 1, 2\n",
      "van: 1\n",
      "vancouver: 2\n",
      "vanessa: 2\n",
      "vantages: 3\n",
      "variances: 2\n",
      "various: 2\n",
      "vasudevan: 1\n",
      "vaswani: 1, 3\n",
      "vector: 1, 2\n",
      "vectors: 1, 2\n",
      "vegas: 1\n",
      "vehicle: 1\n",
      "vehicles: 2\n",
      "venugopalan: 3\n",
      "verberne: 2\n",
      "verma: 2\n",
      "versa: 3\n",
      "very: 1, 2\n",
      "vgg: 1\n",
      "vi: 1, 3\n",
      "via: 1\n",
      "vic: 2\n",
      "vice: 3\n",
      "vicent: 1\n",
      "video: 1, 3\n",
      "videoclip: 3\n",
      "videos: 1, 3\n",
      "view: 2\n",
      "vijay: 1\n",
      "viju: 2\n",
      "vijusudhi: 2\n",
      "village: 1\n",
      "vinced: 2\n",
      "virtex: 3\n",
      "virtual: 1\n",
      "vishwanathan: 2\n",
      "vision: 1, 3\n",
      "visual: 1, 3\n",
      "visualization: 2\n",
      "visualizations: 2\n",
      "visualizing: 2\n",
      "visualsparta: 1\n",
      "vj: 1\n",
      "vk: 1\n",
      "volume: 1, 2\n",
      "von: 2\n",
      "vpu: 1\n",
      "vse: 1\n",
      "vsrn: 1\n",
      "vtt: 3\n",
      "vvt: 1\n",
      "vw–¥: 1\n",
      "w: 2\n",
      "wacv: 1\n",
      "wage: 2\n",
      "waiting: 1\n",
      "wake: 2\n",
      "wallace: 2\n",
      "wallach: 2\n",
      "wang: 1, 2, 3\n",
      "want: 3\n",
      "wanxiang: 2\n",
      "was: 1, 2\n",
      "wasted: 1\n",
      "watching: 3\n",
      "we: 1, 2, 3\n",
      "weakly: 3\n",
      "web: 2\n",
      "website: 1\n",
      "wehnert: 2\n",
      "wei: 1, 2\n",
      "weight: 1\n",
      "weights: 1, 3\n",
      "weissenborn: 3\n",
      "well: 1, 2, 3\n",
      "wen: 1, 2\n",
      "wenhao: 1\n",
      "were: 2, 3\n",
      "what: 1, 3\n",
      "when: 1, 2\n",
      "where: 1, 2, 3\n",
      "which: 1, 2, 3\n",
      "while: 1, 2, 3\n",
      "who: 2\n",
      "whole: 1, 3\n",
      "why: 2\n",
      "widely: 3\n",
      "wikipedia: 2\n",
      "wilcoxon: 2\n",
      "will: 1, 3\n",
      "william: 2\n",
      "winter: 1\n",
      "wise: 1\n",
      "with: 1, 2, 3\n",
      "within: 1, 2\n",
      "without: 1, 2, 3\n",
      "wook: 3\n",
      "word: 1, 2\n",
      "words: 1, 2, 3\n",
      "work: 1, 2, 3\n",
      "worked: 1\n",
      "works: 1, 2\n",
      "workshops: 3\n",
      "world: 3\n",
      "worth: 2, 3\n",
      "would: 3\n",
      "wr: 1\n",
      "wr√≥blewska: 2\n",
      "wsdm: 2\n",
      "wsr: 2\n",
      "www: 2\n",
      "wŒ∏vj: 1\n",
      "wœÉ: 1\n",
      "wœÉvvt: 1\n",
      "wœïvi: 1\n",
      "w–¥: 1\n",
      "x: 1\n",
      "xdc05000000: 1\n",
      "xi: 1, 3\n",
      "xia: 3\n",
      "xiangyu: 1\n",
      "xiao: 1\n",
      "xiaochun: 1\n",
      "xiaodong: 1\n",
      "xiaogang: 1\n",
      "xiaopeng: 1\n",
      "xiaoping: 1\n",
      "xiaowei: 1\n",
      "xihui: 1\n",
      "xilin: 1\n",
      "xing: 1\n",
      "xingjun: 3\n",
      "xiujun: 1\n",
      "xu: 1, 3\n",
      "xueqi: 2\n",
      "xùë°: 3\n",
      "xùë°is: 3\n",
      "xùë°ùëõ: 3\n",
      "xùë£: 3\n",
      "yan: 1\n",
      "yang: 1, 2, 3\n",
      "yannis: 3\n",
      "yanzhuo: 2\n",
      "yao: 1, 3\n",
      "yarali: 2\n",
      "ye: 1, 3\n",
      "yehia: 1\n",
      "yejin: 1\n",
      "yen: 1, 3\n",
      "yet: 2\n",
      "yi: 1, 2, 3\n",
      "yield: 2\n",
      "yields: 2\n",
      "yikang: 3\n",
      "yin: 1\n",
      "yinfei: 3\n",
      "yiqun: 2\n",
      "yisen: 3\n",
      "yixing: 2\n",
      "yoelle: 2\n",
      "yong: 3\n",
      "york: 1, 2, 3\n",
      "yoshua: 1\n",
      "you: 1, 2, 3\n",
      "young: 1\n",
      "youngjae: 3\n",
      "youtube: 3\n",
      "ysbr: 1\n",
      "yu: 1, 3\n",
      "yuan: 1, 3\n",
      "yuanning: 2\n",
      "yuanyuan: 1\n",
      "yuke: 3\n",
      "yulun: 1\n",
      "yun: 1, 2\n",
      "yunhsuan: 3\n",
      "yutai: 2\n",
      "yuwei: 1\n",
      "yuxuan: 2\n",
      "yùë°: 3\n",
      "yùë£: 3\n",
      "zaiyi: 3\n",
      "zarana: 3\n",
      "zeon: 2\n",
      "zequn: 1\n",
      "zhai: 3\n",
      "zhang: 1, 2, 3\n",
      "zhao: 1\n",
      "zhe: 3\n",
      "zhen: 3\n",
      "zheng: 1\n",
      "zhilan: 1\n",
      "zhixin: 1\n",
      "zhou: 3\n",
      "zhu: 1, 3\n",
      "zhukov: 3\n",
      "zihao: 1\n",
      "zisserman: 1, 3\n",
      "zitnick: 1\n",
      "ziwei: 1\n",
      "zurich: 1\n",
      "zùë°: 3\n",
      "zùë£: 3\n",
      "√©ric: 2\n",
      "√≠: 2\n",
      "√≠ùëÄexpùëÜ: 3\n",
      "≈Çukasz: 3\n",
      "Œ≥ùë°: 3\n",
      "Œ≥ùë°is: 3\n",
      "Œ≥ùë£: 3\n",
      "Œ≥ùë£is: 3\n",
      "Œ∏: 1\n",
      "œÉ: 1\n",
      "œï: 1\n",
      "ùêµ: 3\n",
      "ùêµis: 3\n",
      "ùê∂ùêøùëÜ: 3\n",
      "ùêº1: 3\n",
      "ùêºùëõ: 3\n",
      "ùêæ: 3\n",
      "ùêædepth: 3\n",
      "ùêæequals: 3\n",
      "ùêætimes: 3\n",
      "ùëÄ: 3\n",
      "ùëÄis: 3\n",
      "ùëÄùëëùëÖ: 3\n",
      "ùëÄùëíùëéùëõ: 3\n",
      "ùëÄùëõùëÖ: 3\n",
      "ùëÅ: 3\n",
      "ùëÖ: 3\n",
      "ùëÜ: 3\n",
      "ùëÜis: 3\n",
      "ùëá: 3\n",
      "ùëäuser: 2\n",
      "ùëé: 2\n",
      "ùëéand: 2\n",
      "ùëéùë†ùë†ùëúùëêùëñùëéùë°ùëñùëúùëõ: 2\n",
      "ùëè: 2\n",
      "ùëêùëôùë†: 3\n",
      "ùëêùëôùë†and: 3\n",
      "ùëêùëôùë†represents: 3\n",
      "ùëêùëôùë†together: 3\n",
      "ùëêùëú: 2\n",
      "ùëë: 2\n",
      "ùëìùë°: 3\n",
      "ùëìùë£: 3\n",
      "ùëîùë°: 3\n",
      "ùëîùë£: 3\n",
      "ùëñ: 3\n",
      "ùëôùëñ: 3\n",
      "ùëôùëñlog: 3\n",
      "ùëôùëúùëîexp: 3\n",
      "ùëõ: 3\n",
      "ùëõùëêùëôùë†: 3\n",
      "ùëúùëêùëêùë¢ùëüùëüùëíùëõùëêùëí: 2\n",
      "ùëù: 2, 3\n",
      "ùëû: 2\n",
      "ùëûand: 2\n",
      "ùëûfor: 2\n",
      "ùëûis: 2\n",
      "ùë†ùëñùëîùëõùëñùëìùëñùëêùëéùëõùëêùëí: 2\n",
      "ùë†ùëñùëö: 2\n",
      "ùë†ùëúùëìùë°ùëöùëéùë•: 3\n",
      "ùë†ùë°ùëû: 2\n",
      "ùë†ùë•: 2\n",
      "ùë°: 3\n",
      "ùë°are: 3\n",
      "ùë°is: 3\n",
      "ùë°ùëë: 2\n",
      "ùë°ùëëis: 2\n",
      "ùë°ùëëwith: 2\n",
      "ùë°ùëû: 2\n",
      "ùë°ùëûwith: 2\n",
      "ùë£: 3\n",
      "ùë£and: 3\n",
      "ùë§factorùë§: 2\n",
      "ùë§is: 2\n",
      "ùë§significant: 2\n",
      "ùë•: 2\n",
      "ùë•and: 2\n",
      "ùë•in: 2\n",
      "ùë•is: 2\n",
      "ùë•ùë£: 3\n",
      "ùë•ùë£ùëõùëñwhere: 3\n",
      "ùõº: 2\n",
      "ùúÇ: 2\n",
      "ùúé: 2\n",
      "ùúè: 3\n",
      "ùúèis: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input the folder path containing your .txt files.\n",
    "    folder_path = r'C:\\Users\\Public\\Dev\\Ph.D\\2nd Semester\\CS-675-IRS\\Assignments\\First\\TXT 1-250-1'\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The folder path does not exist. Please check the path.\")\n",
    "    else:\n",
    "        inverted_index = build_inverted_index(folder_path)\n",
    "        print_inverted_index(inverted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Convert text to lowercase and extract alphanumeric tokens.\n",
    "    Uses a regular expression to extract words (letters and digits).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "def build_inverted_index(folder_path):\n",
    "    \"\"\"\n",
    "    Build an inverted index from all .txt files in the folder.\n",
    "    \n",
    "    Each token (term) maps to a set of document IDs (assigned based on the file processing order).\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(set)\n",
    "    doc_id = 1  # Document IDs start at 1\n",
    "\n",
    "    # Process files in sorted order to ensure reproducibility in doc_id assignment.\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        print(f'Tokenizing file: {filename}')\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            tokens = tokenize(text)\n",
    "            for token in tokens:\n",
    "                inverted_index[token].add(doc_id)\n",
    "            \n",
    "            doc_id += 1\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "def store_inverted_index(inverted_index, output_filename):\n",
    "    \"\"\"\n",
    "    Write the inverted index into a file with the specified format.\n",
    "    \n",
    "    Format:\n",
    "      INVERTED INDEX\n",
    "      ==============\n",
    "      Format: term: doc1, doc2, ...\n",
    "      \n",
    "      <term>: <doc id list>\n",
    "      ...\n",
    "    \"\"\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INVERTED INDEX\\n\")\n",
    "        f.write(\"==============\\n\")\n",
    "        f.write(\"Format: term: doc1, doc2, ...\\n\\n\")\n",
    "        \n",
    "        # Sort tokens for a consistent output order.\n",
    "        for term in sorted(inverted_index.keys()):\n",
    "            # Convert document id set to a sorted list.\n",
    "            doc_ids = sorted(list(inverted_index[term]))\n",
    "            doc_ids_str = \", \".join(str(id) for id in doc_ids)\n",
    "            f.write(f\"{term}: {doc_ids_str}\\n\")\n",
    "    \n",
    "    print(f\"Inverted index stored in '{output_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing file: 1.txt\n",
      "Tokenizing file: 10.txt\n",
      "Tokenizing file: 100.txt\n",
      "Tokenizing file: 108.txt\n",
      "Tokenizing file: 109.txt\n",
      "Tokenizing file: 11.txt\n",
      "Tokenizing file: 110.txt\n",
      "Tokenizing file: 111.txt\n",
      "Tokenizing file: 112.txt\n",
      "Tokenizing file: 113.txt\n",
      "Tokenizing file: 114.txt\n",
      "Tokenizing file: 115.txt\n",
      "Tokenizing file: 116.txt\n",
      "Tokenizing file: 117.txt\n",
      "Tokenizing file: 118.txt\n",
      "Tokenizing file: 119.txt\n",
      "Tokenizing file: 12.txt\n",
      "Tokenizing file: 120.txt\n",
      "Tokenizing file: 121.txt\n",
      "Tokenizing file: 122.txt\n",
      "Tokenizing file: 123.txt\n",
      "Tokenizing file: 124.txt\n",
      "Tokenizing file: 125.txt\n",
      "Tokenizing file: 126.txt\n",
      "Tokenizing file: 127.txt\n",
      "Tokenizing file: 128.txt\n",
      "Tokenizing file: 129.txt\n",
      "Tokenizing file: 13.txt\n",
      "Tokenizing file: 130.txt\n",
      "Tokenizing file: 131.txt\n",
      "Tokenizing file: 132.txt\n",
      "Tokenizing file: 133.txt\n",
      "Tokenizing file: 134.txt\n",
      "Tokenizing file: 135.txt\n",
      "Tokenizing file: 136.txt\n",
      "Tokenizing file: 137.txt\n",
      "Tokenizing file: 138.txt\n",
      "Tokenizing file: 139.txt\n",
      "Tokenizing file: 14.txt\n",
      "Tokenizing file: 140.txt\n",
      "Tokenizing file: 141.txt\n",
      "Tokenizing file: 142.txt\n",
      "Tokenizing file: 15.txt\n",
      "Tokenizing file: 151.txt\n",
      "Tokenizing file: 152.txt\n",
      "Tokenizing file: 153.txt\n",
      "Tokenizing file: 154.txt\n",
      "Tokenizing file: 155.txt\n",
      "Tokenizing file: 156.txt\n",
      "Tokenizing file: 157.txt\n",
      "Tokenizing file: 16.txt\n",
      "Tokenizing file: 17.txt\n",
      "Tokenizing file: 173.txt\n",
      "Tokenizing file: 174.txt\n",
      "Tokenizing file: 175.txt\n",
      "Tokenizing file: 176.txt\n",
      "Tokenizing file: 177.txt\n",
      "Tokenizing file: 178.txt\n",
      "Tokenizing file: 179.txt\n",
      "Tokenizing file: 18.txt\n",
      "Tokenizing file: 180.txt\n",
      "Tokenizing file: 188.txt\n",
      "Tokenizing file: 189.txt\n",
      "Tokenizing file: 19.txt\n",
      "Tokenizing file: 190.txt\n",
      "Tokenizing file: 191.txt\n",
      "Tokenizing file: 192.txt\n",
      "Tokenizing file: 193.txt\n",
      "Tokenizing file: 194.txt\n",
      "Tokenizing file: 195.txt\n",
      "Tokenizing file: 196.txt\n",
      "Tokenizing file: 197.txt\n",
      "Tokenizing file: 198.txt\n",
      "Tokenizing file: 199.txt\n",
      "Tokenizing file: 2.txt\n",
      "Tokenizing file: 20.txt\n",
      "Tokenizing file: 200.txt\n",
      "Tokenizing file: 21.txt\n",
      "Tokenizing file: 214.txt\n",
      "Tokenizing file: 215.txt\n",
      "Tokenizing file: 216.txt\n",
      "Tokenizing file: 217.txt\n",
      "Tokenizing file: 218.txt\n",
      "Tokenizing file: 219.txt\n",
      "Tokenizing file: 22.txt\n",
      "Tokenizing file: 220.txt\n",
      "Tokenizing file: 221.txt\n",
      "Tokenizing file: 222.txt\n",
      "Tokenizing file: 223.txt\n",
      "Tokenizing file: 224.txt\n",
      "Tokenizing file: 225.txt\n",
      "Tokenizing file: 226.txt\n",
      "Tokenizing file: 227.txt\n",
      "Tokenizing file: 228.txt\n",
      "Tokenizing file: 229.txt\n",
      "Tokenizing file: 23.txt\n",
      "Tokenizing file: 230.txt\n",
      "Tokenizing file: 231.txt\n",
      "Tokenizing file: 232.txt\n",
      "Tokenizing file: 233.txt\n",
      "Tokenizing file: 234.txt\n",
      "Tokenizing file: 235.txt\n",
      "Tokenizing file: 236.txt\n",
      "Tokenizing file: 237.txt\n",
      "Tokenizing file: 238.txt\n",
      "Tokenizing file: 24.txt\n",
      "Tokenizing file: 25.txt\n",
      "Tokenizing file: 26.txt\n",
      "Tokenizing file: 27.txt\n",
      "Tokenizing file: 28.txt\n",
      "Tokenizing file: 29.txt\n",
      "Tokenizing file: 3.txt\n",
      "Tokenizing file: 30.txt\n",
      "Tokenizing file: 31.txt\n",
      "Tokenizing file: 32.txt\n",
      "Tokenizing file: 33.txt\n",
      "Tokenizing file: 34.txt\n",
      "Tokenizing file: 35.txt\n",
      "Tokenizing file: 36.txt\n",
      "Tokenizing file: 37.txt\n",
      "Tokenizing file: 38.txt\n",
      "Tokenizing file: 39.txt\n",
      "Tokenizing file: 4.txt\n",
      "Tokenizing file: 40.txt\n",
      "Tokenizing file: 41.txt\n",
      "Tokenizing file: 42.txt\n",
      "Tokenizing file: 43.txt\n",
      "Tokenizing file: 44.txt\n",
      "Tokenizing file: 45.txt\n",
      "Tokenizing file: 46.txt\n",
      "Tokenizing file: 47.txt\n",
      "Tokenizing file: 48.txt\n",
      "Tokenizing file: 49.txt\n",
      "Tokenizing file: 5.txt\n",
      "Tokenizing file: 50.txt\n",
      "Tokenizing file: 51.txt\n",
      "Tokenizing file: 52.txt\n",
      "Tokenizing file: 53.txt\n",
      "Tokenizing file: 54.txt\n",
      "Tokenizing file: 55.txt\n",
      "Tokenizing file: 56.txt\n",
      "Tokenizing file: 57.txt\n",
      "Tokenizing file: 58.txt\n",
      "Tokenizing file: 59.txt\n",
      "Tokenizing file: 6.txt\n",
      "Tokenizing file: 60.txt\n",
      "Tokenizing file: 61.txt\n",
      "Tokenizing file: 62.txt\n",
      "Tokenizing file: 63.txt\n",
      "Tokenizing file: 64.txt\n",
      "Tokenizing file: 65.txt\n",
      "Tokenizing file: 66.txt\n",
      "Tokenizing file: 67.txt\n",
      "Tokenizing file: 68.txt\n",
      "Tokenizing file: 69.txt\n",
      "Tokenizing file: 7.txt\n",
      "Tokenizing file: 70.txt\n",
      "Tokenizing file: 71.txt\n",
      "Tokenizing file: 72.txt\n",
      "Tokenizing file: 73.txt\n",
      "Tokenizing file: 74.txt\n",
      "Tokenizing file: 75.txt\n",
      "Tokenizing file: 76.txt\n",
      "Tokenizing file: 77.txt\n",
      "Tokenizing file: 78.txt\n",
      "Tokenizing file: 79.txt\n",
      "Tokenizing file: 8.txt\n",
      "Tokenizing file: 80.txt\n",
      "Tokenizing file: 81.txt\n",
      "Tokenizing file: 86.txt\n",
      "Tokenizing file: 87.txt\n",
      "Tokenizing file: 88.txt\n",
      "Tokenizing file: 89.txt\n",
      "Tokenizing file: 9.txt\n",
      "Tokenizing file: 90.txt\n",
      "Tokenizing file: 91.txt\n",
      "Tokenizing file: 92.txt\n",
      "Tokenizing file: 93.txt\n",
      "Tokenizing file: 94.txt\n",
      "Tokenizing file: 95.txt\n",
      "Tokenizing file: 96.txt\n",
      "Tokenizing file: 97.txt\n",
      "Tokenizing file: 98.txt\n",
      "Tokenizing file: 99.txt\n",
      "Inverted index stored in 'inverted_index.txt'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input the folder path containing your .txt files.\n",
    "    # folder_path = input(\"Enter the folder path containing the .txt files: \").strip()\n",
    "    folder_path = r'C:\\Users\\Public\\Dev\\Ph.D\\2nd Semester\\CS-675-IRS\\Assignments\\First\\TXT 1-250'\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The folder path does not exist. Please check the path.\")\n",
    "    else:\n",
    "        inverted_index = build_inverted_index(folder_path)\n",
    "        output_file = \"inverted_index.txt\"  # You can change the output file name if needed.\n",
    "        store_inverted_index(inverted_index, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: C:\\Users\\Public\\Dev\\Ph.D\\2nd Semester\\CS-675-IRS\\Assignments\\First\\TXT 1-250\n",
      "Path exists? True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Convert text to lowercase and extract alphanumeric tokens.\n",
    "    Uses a regular expression to extract words (letters and digits).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "def get_doc_id_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract the numeric part from a filename like '1.txt' or '123.txt'.\n",
    "    Returns an integer (e.g., 1, 123).\n",
    "    Assumes the filename is strictly of the form '<number>.txt'.\n",
    "    \"\"\"\n",
    "    # Split off the extension, e.g. \"1.txt\" -> (\"1\", \".txt\")\n",
    "    name_part, ext = os.path.splitext(filename)\n",
    "    # Convert the name_part (\"1\", \"7\", etc.) to an integer.\n",
    "    return int(name_part)  # This will raise ValueError if name_part isn't numeric\n",
    "\n",
    "def build_inverted_index(folder_path):\n",
    "    \"\"\"\n",
    "    Build an inverted index from all .txt files in the folder,\n",
    "    using the filename's numeric part as the doc_id (e.g. \"7.txt\" -> doc_id = 7).\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(set)\n",
    "\n",
    "    # Process files in sorted order so the output is consistent.\n",
    "    # Sorting ensures \"1.txt\" comes before \"2.txt\" < ... < \"10.txt\", etc.\n",
    "    # (Though note that string-sorting might place \"10.txt\" after \"1.txt\" before \"2.txt\". \n",
    "    #  If you want pure numeric sorting, see the note below.)\n",
    "    files = sorted(f for f in os.listdir(folder_path) if f.endswith(\".txt\"))\n",
    "\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        doc_id = get_doc_id_from_filename(filename)  # preserve the file's numeric ID\n",
    "\n",
    "        # Read the file text\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenize(text)\n",
    "\n",
    "        # Populate the inverted index\n",
    "        for token in tokens:\n",
    "            inverted_index[token].add(doc_id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "def store_inverted_index(inverted_index, output_filename):\n",
    "    \"\"\"\n",
    "    Write the inverted index into a file with the specified format:\n",
    "    \n",
    "    INVERTED INDEX\n",
    "    ==============\n",
    "    Format: term: doc1, doc2, ...\n",
    "    \n",
    "    <term>: <doc id list>\n",
    "    ...\n",
    "    \"\"\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INVERTED INDEX\\n\")\n",
    "        f.write(\"==============\\n\")\n",
    "        f.write(\"Format: term: doc1, doc2, ...\\n\\n\")\n",
    "        \n",
    "        # Sort tokens for a consistent output order.\n",
    "        for term in sorted(inverted_index.keys()):\n",
    "            # Convert the set of doc IDs to a sorted list.\n",
    "            doc_ids = sorted(list(inverted_index[term]))\n",
    "            doc_ids_str = \", \".join(str(did) for did in doc_ids)\n",
    "            f.write(f\"{term}: {doc_ids_str}\\n\")\n",
    "    \n",
    "    print(f\"Inverted index stored in '{output_filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index stored in 'inverted_index.txt'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # folder_path = input(\"Enter the folder path containing the .txt files: \").strip()\n",
    "    folder_path = r'C:\\Users\\Public\\Dev\\Ph.D\\2nd Semester\\CS-675-IRS\\Assignments\\First\\TXT 1-250'\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"The folder path does not exist. Please check the path.\")\n",
    "    else:\n",
    "        inverted_index = build_inverted_index(folder_path)\n",
    "        output_file = \"inverted_index.txt\"\n",
    "        store_inverted_index(inverted_index, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
